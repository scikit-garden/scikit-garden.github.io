{
    "docs": [
        {
            "location": "/",
            "text": "Scikit-garden\n\n\nScikit-garden or skgarden (pronounced as skarden) is a garden for scikit-learn compatible trees.\n\n\nInstallation\n\n\nScikit-Garden depends on NumPy, SciPy, Scikit-Learn and Cython. So make sure these dependencies are installed using pip:\n\n\npip install setuptools numpy scipy scikit-learn cython\n\n\n\n\nAfter that Scikit-Garden can be installed using pip.\n\n\npip install scikit-garden\n\n\n\n\nUsage\n\n\nThe estimators in Scikit-Garden are Scikit-Learn compatible and can serve as a drop-in replacement for Scikit-Learn's trees and forests.\n\n\nfrom sklearn.datasets import load_boston\nX, y = make_boston()\n\n### Use MondrianForests for variance estimation\nfrom skgarden import MondrianForestRegressor\nmfr = MondrianForestRegressor()\nmfr.fit(X, y)\ny_mean, y_std = mfr.predict(X, return_std=True)\n\n### Use QuantileForests for quantile estimation\nfrom skgarden import RandomForestQuantileRegressor\nrfqr = RandomForestQuantileRegressor(random_state=0)\nrfqr.fit(X, y)\ny_mean = rfqr.predict(X)\ny_median = rfqr.predict(X, 50)",
            "title": "Home"
        },
        {
            "location": "/#scikit-garden",
            "text": "Scikit-garden or skgarden (pronounced as skarden) is a garden for scikit-learn compatible trees.",
            "title": "Scikit-garden"
        },
        {
            "location": "/#installation",
            "text": "Scikit-Garden depends on NumPy, SciPy, Scikit-Learn and Cython. So make sure these dependencies are installed using pip:  pip install setuptools numpy scipy scikit-learn cython  After that Scikit-Garden can be installed using pip.  pip install scikit-garden",
            "title": "Installation"
        },
        {
            "location": "/#usage",
            "text": "The estimators in Scikit-Garden are Scikit-Learn compatible and can serve as a drop-in replacement for Scikit-Learn's trees and forests.  from sklearn.datasets import load_boston\nX, y = make_boston()\n\n### Use MondrianForests for variance estimation\nfrom skgarden import MondrianForestRegressor\nmfr = MondrianForestRegressor()\nmfr.fit(X, y)\ny_mean, y_std = mfr.predict(X, return_std=True)\n\n### Use QuantileForests for quantile estimation\nfrom skgarden import RandomForestQuantileRegressor\nrfqr = RandomForestQuantileRegressor(random_state=0)\nrfqr.fit(X, y)\ny_mean = rfqr.predict(X)\ny_median = rfqr.predict(X, 50)",
            "title": "Usage"
        },
        {
            "location": "/api/",
            "text": "API documentation of skgarden\n\n\nTable of contents\n\n\nskgarden.mondrian\n\n\n\n\nskgarden.mondrian.MondrianForestClassifier\n\n\nskgarden.mondrian.MondrianForestRegressor\n\n\nskgarden.mondrian.MondrianTreeClassifier\n\n\nskgarden.mondrian.MondrianTreeRegressor\n\n\n\n\nskgarden.quantile\n\n\n\n\nskgarden.quantile.DecisionTreeQuantileRegressor\n\n\nskgarden.quantile.ExtraTreeQuantileRegressor\n\n\nskgarden.quantile.ExtraTreesQuantileRegressor\n\n\nskgarden.quantile.RandomForestQuantileRegressor\n\n\n\n\nskgarden.forest\n\n\n\n\nskgarden.forest.ExtraTreesRegressor\n\n\nskgarden.forest.RandomForestRegressor\n\n\n\n\nskgarden.mondrian\n\n\nskgarden.mondrian.MondrianForestClassifier\n\n\nA MondrianForestClassifier is an ensemble of MondrianTreeClassifiers.\n\n\nThe probability \np_{j}\n of class \nj\n is given\n\n\\sum_{i}^{N_{est}} \\frac{p_{j}^i}{N_{est}}\n\n\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\nmax_depth\n (integer, optional (default=None))\n\n\nThe depth to which each tree is grown. If None, the tree is either\ngrown to full depth or is constrained by \nmin_samples_split\n.\n\n\n\n\n\n\nmin_samples_split\n (integer, optional (default=2))\n\n\nStop growing the tree if all the nodes have lesser than\n\nmin_samples_split\n number of samples.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=False))\n\n\nIf bootstrap is set to False, then all trees are trained on the\nentire training dataset. Else, each tree is fit on n_samples\ndrawn with replacement from the training dataset.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nMethods\n\n\nMondrianForestClassifier.fit(X, y)\n\n\nBuilds a forest of trees from the training set (X, y).\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe training input samples. Internally, its dtype will be converted\nto \ndtype=np.float32\n. If a sparse matrix is provided, it will be\nconverted into a sparse \ncsc_matrix\n.\n\n\n\n\n\n\ny\n (array-like, shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe target values (class labels in classification, real numbers in\nregression).\n\n\n\n\n\n\nsample_weight\n (array-like, shape = [n_samples] or None)\n\n\nSample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nself\n (object)\n\n\nReturns self.\n\n\n\n\n\n\nMondrianForestClassifier.weighted_decision_path(X)\n\n\nReturns the weighted decision path in the forest.\n\n\nEach non-zero value in the decision path determines the\nweight of that particular node while making predictions.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like, shape = (n_samples, n_features))\n\n\nInput.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndecision_path\n (sparse csr matrix, shape = (n_samples, n_total_nodes))\n\n\nReturn a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.\n\n\n\n\n\n\nest_inds\n (array-like, shape = (n_estimators + 1,))\n\n\nweighted_decision_path[:, est_inds[i]: est_inds[i + 1]]\nprovides the weighted_decision_path of estimator i\n\n\n\n\n\n\nProperties\n\n\nskgarden.mondrian.MondrianForestRegressor\n\n\nA MondrianForestRegressor is an ensemble of MondrianTreeRegressors.\n\n\nThe variance in predictions is reduced by averaging the predictions\nfrom all trees.\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\nmax_depth\n (integer, optional (default=None))\n\n\nThe depth to which each tree is grown. If None, the tree is either\ngrown to full depth or is constrained by \nmin_samples_split\n.\n\n\n\n\n\n\nmin_samples_split\n (integer, optional (default=2))\n\n\nStop growing the tree if all the nodes have lesser than\n\nmin_samples_split\n number of samples.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=False))\n\n\nIf bootstrap is set to False, then all trees are trained on the\nentire training dataset. Else, each tree is fit on n_samples\ndrawn with replacement from the training dataset.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nMethods\n\n\nMondrianForestRegressor.fit(X, y)\n\n\nBuilds a forest of trees from the training set (X, y).\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe training input samples. Internally, its dtype will be converted\nto \ndtype=np.float32\n. If a sparse matrix is provided, it will be\nconverted into a sparse \ncsc_matrix\n.\n\n\n\n\n\n\ny\n (array-like, shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe target values (class labels in classification, real numbers in\nregression).\n\n\n\n\n\n\nsample_weight\n (array-like, shape = [n_samples] or None)\n\n\nSample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nself\n (object)\n\n\nReturns self.\n\n\n\n\n\n\nMondrianForestRegressor.predict(X, return_std=False)\n\n\nReturns the predicted mean and std.\n\n\nThe prediction is a GMM drawn from\n\n\\sum_{i=1}^T w_i N(m_i, \\sigma_i)\n where \nw_i = {1 \\over T}\n.\n\n\nThe mean \nE[Y | X]\n reduces to \n{\\sum_{i=1}^T m_i \\over T}\n\n\n\n\nThe variance \nVar[Y | X]\n is given by \nVar[Y | X] = E[Y^2 | X] - E[Y | X]^2\n\n\n=\\frac{\\sum_{i=1}^T E[Y^2_i| X]}{T} - E[Y | X]^2\n\n\n= \\frac{\\sum_{i=1}^T (Var[Y_i | X] + E[Y_i | X]^2)}{T} - E[Y| X]^2\n\n\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like, shape = (n_samples, n_features))\n\n\nInput samples.\n\n\n\n\n\n\nreturn_std\n (boolean, default (False))\n\n\nWhether or not to return the standard deviation.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array-like, shape = (n_samples,))\n\n\nPredictions at X.\n\n\n\n\n\n\nstd\n (array-like, shape = (n_samples,))\n\n\nStandard deviation at X.\n\n\n\n\n\n\nMondrianForestRegressor.weighted_decision_path(X)\n\n\nReturns the weighted decision path in the forest.\n\n\nEach non-zero value in the decision path determines the\nweight of that particular node while making predictions.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like, shape = (n_samples, n_features))\n\n\nInput.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ndecision_path\n (sparse csr matrix, shape = (n_samples, n_total_nodes))\n\n\nReturn a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.\n\n\n\n\n\n\nest_inds\n (array-like, shape = (n_estimators + 1,))\n\n\nweighted_decision_path[:, est_inds[i]: est_inds[i + 1]]\nprovides the weighted_decision_path of estimator i\n\n\n\n\n\n\nProperties\n\n\nskgarden.mondrian.MondrianTreeClassifier\n\n\nA Mondrian tree.\n\n\nThe splits in a mondrian tree regressor differ from the standard regression\ntree in the following ways.\n\n\nAt fit time:\n    - Splits are done independently of the labels.\n    - The candidate feature is drawn with a probability proportional to the\n      feature range.\n    - The candidate threshold is drawn from a uniform distribution\n      with the bounds equal to the bounds of the candidate feature.\n    - The time of split is also stored which is proportional to the\n      inverse of the size of the bounding-box.\n\n\nAt prediction time:\n    - Every node in the path from the root to the leaf is given a weight\n      while making predictions.\n    - At each node, the probability of an unseen sample splitting from that\n      node is calculated. The farther the sample is away from the bounding\n      box, the more probable that it will split away.\n    - For every node, the probability that an unseen sample has not split\n      before reaching that node and the probability that it will split away\n      at that particular node are multiplied to give a weight.\n\n\nParameters\n\n\n\n\n\n\nmax_depth\n (int or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n\n\n\n\nIf int, then consider \nmin_samples_split\n as the minimum number.\n\n\nIf float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n\n\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nMethods\n\n\nMondrianTreeClassifier.apply(X, check_input=True)\n\n\nReturns the index of the leaf that each sample is predicted as.\n\n\n.. versionadded:: 0.17\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_leaves\n (array_like, shape = [n_samples,])\n\n\nFor each datapoint x in X, return the index of the leaf x\nends up in. Leaves are numbered within\n\n[0; self.tree_.node_count)\n, possibly with gaps in the\nnumbering.\n\n\n\n\n\n\nMondrianTreeClassifier.decision_path(X, check_input=True)\n\n\nReturn the decision path in the tree\n\n\n.. versionadded:: 0.18\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nindicator\n (sparse csr array, shape = [n_samples, n_nodes])\n\n\nReturn a node indicator matrix where non zero elements\nindicates that the samples goes through the nodes.\n\n\n\n\n\n\nMondrianTreeClassifier.fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n\n\nMondrianTreeClassifier.predict(X, check_input=True, return_std=False)\n\n\nPredict class or regression value for X.\n\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nreturn_std\n (boolean, (default=True))\n\n\nWhether or not to return the standard deviation.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe predicted classes, or the predict values.\n\n\n\n\n\n\nMondrianTreeClassifier.predict_proba(X, check_input=True)\n\n\nPredicts the probability of each class label given X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny_prob\n (array of shape = [n_samples, n_classes])\n\n\nPrediceted probabilities for each class.\n\n\n\n\n\n\nMondrianTreeClassifier.weighted_decision_path(X, check_input=True)\n\n\nReturns the weighted decision path in the tree.\n\n\nEach non-zero value in the decision path determines the weight\nof that particular node in making predictions.\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nindicator\n (sparse csr array, shape = [n_samples, n_nodes])\n\n\nReturn a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.\n\n\n\n\n\n\nProperties\n\n\nskgarden.mondrian.MondrianTreeRegressor\n\n\nA Mondrian tree.\n\n\nThe splits in a mondrian tree regressor differ from the standard regression\ntree in the following ways.\n\n\nAt fit time:\n    - Splits are done independently of the labels.\n    - The candidate feature is drawn with a probability proportional to the\n      feature range.\n    - The candidate threshold is drawn from a uniform distribution\n      with the bounds equal to the bounds of the candidate feature.\n    - The time of split is also stored which is proportional to the\n      inverse of the size of the bounding-box.\n\n\nAt prediction time:\n    - Every node in the path from the root to the leaf is given a weight\n      while making predictions.\n    - At each node, the probability of an unseen sample splitting from that\n      node is calculated. The farther the sample is away from the bounding\n      box, the more probable that it will split away.\n    - For every node, the probability that an unseen sample has not split\n      before reaching that node and the probability that it will split away\n      at that particular node are multiplied to give a weight.\n\n\nParameters\n\n\n\n\n\n\nmax_depth\n (int or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n\n\n\n\nIf int, then consider \nmin_samples_split\n as the minimum number.\n\n\nIf float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n\n\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nMethods\n\n\nMondrianTreeRegressor.apply(X, check_input=True)\n\n\nReturns the index of the leaf that each sample is predicted as.\n\n\n.. versionadded:: 0.17\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nX_leaves\n (array_like, shape = [n_samples,])\n\n\nFor each datapoint x in X, return the index of the leaf x\nends up in. Leaves are numbered within\n\n[0; self.tree_.node_count)\n, possibly with gaps in the\nnumbering.\n\n\n\n\n\n\nMondrianTreeRegressor.decision_path(X, check_input=True)\n\n\nReturn the decision path in the tree\n\n\n.. versionadded:: 0.18\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nindicator\n (sparse csr array, shape = [n_samples, n_nodes])\n\n\nReturn a node indicator matrix where non zero elements\nindicates that the samples goes through the nodes.\n\n\n\n\n\n\nMondrianTreeRegressor.fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n\n\nMondrianTreeRegressor.predict(X, check_input=True, return_std=False)\n\n\nPredict class or regression value for X.\n\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nreturn_std\n (boolean, (default=True))\n\n\nWhether or not to return the standard deviation.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe predicted classes, or the predict values.\n\n\n\n\n\n\nMondrianTreeRegressor.weighted_decision_path(X, check_input=True)\n\n\nReturns the weighted decision path in the tree.\n\n\nEach non-zero value in the decision path determines the weight\nof that particular node in making predictions.\n\n\nParameters\n\n\n\n\n\n\nX\n (array_like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nindicator\n (sparse csr array, shape = [n_samples, n_nodes])\n\n\nReturn a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.\n\n\n\n\n\n\nProperties\n\n\nskgarden.quantile\n\n\nskgarden.quantile.DecisionTreeQuantileRegressor\n\n\nA decision tree regressor that provides quantile estimates.\n\n\nParameters\n\n\n\n\n\n\ncriterion\n (string, optional (default=\"mse\"))\n\n\nThe function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.\n\n\n\n\n\n\nsplitter\n (string, optional (default=\"best\"))\n\n\nThe strategy used to choose the split at each node. Supported\nstrategies are \"best\" to choose the best split and \"random\" to choose\nthe best random split.\n\n\n\n\n\n\nmax_features\n (int, float, string or None, optional (default=None))\n\n\nThe number of features to consider when looking for the best split:\n- If int, then consider \nmax_features\n features at each split.\n- If float, then \nmax_features\n is a percentage and\n  \nint(max_features * n_features)\n features are considered at each\n  split.\n- If \"auto\", then \nmax_features=n_features\n.\n- If \"sqrt\", then \nmax_features=sqrt(n_features)\n.\n- If \"log2\", then \nmax_features=log2(n_features)\n.\n- If None, then \nmax_features=n_features\n.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than \nmax_features\n features.\n\n\n\n\n\n\nmax_depth\n (int or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider \nmin_samples_split\n as the minimum number.\n- If float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_samples_leaf\n (int, float, optional (default=1))\n\n\nThe minimum number of samples required to be at a leaf node:\n- If int, then consider \nmin_samples_leaf\n as the minimum number.\n- If float, then \nmin_samples_leaf\n is a percentage and\n  \nceil(min_samples_leaf * n_samples)\n are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_weight_fraction_leaf\n (float, optional (default=0.))\n\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n\n\n\n\n\n\nmax_leaf_nodes\n (int or None, optional (default=None))\n\n\nGrow a tree with \nmax_leaf_nodes\n in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\npresort\n (bool, optional (default=False))\n\n\nWhether to presort the data to speed up the finding of best splits in\nfitting. For the default settings of a decision tree on large\ndatasets, setting this to true may slow down the training process.\nWhen using either a smaller dataset or a restricted depth, this may\nspeed up the training.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nfeature_importances_\n (array of shape = [n_features])\n\n\nThe feature importances.\nThe higher, the more important the feature.\nThe importance of a feature is computed as the\n(normalized) total reduction of the criterion brought\nby that feature. It is also known as the Gini importance [4]_.\n\n\n\n\n\n\nmax_features_\n (int,)\n\n\nThe inferred value of max_features.\n\n\n\n\n\n\nn_features_\n (int)\n\n\nThe number of features when \nfit\n is performed.\n\n\n\n\n\n\nn_outputs_\n (int)\n\n\nThe number of outputs when \nfit\n is performed.\n\n\n\n\n\n\ntree_\n (Tree object)\n\n\nThe underlying Tree object.\n\n\n\n\n\n\ny_train_\n (array-like)\n\n\nTrain target values.\n\n\n\n\n\n\ny_train_leaves_\n (array-like.)\n\n\nCache the leaf nodes that each training sample falls into.\ny_train_leaves_[i] is the leaf that y_train[i] ends up at.\n\n\n\n\n\n\nMethods\n\n\nDecisionTreeQuantileRegressor.predict(X, quantile=None, check_input=False)\n\n\nPredict regression value for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\nquantile\n (int, optional)\n\n\nValue ranging from 0 to 100. By default, the mean is returned.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples])\n\n\nIf quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.\n\n\n\n\n\n\nProperties\n\n\nskgarden.quantile.ExtraTreeQuantileRegressor\n\n\nAn extremely randomized tree regressor.\n\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the \nmax_features\n randomly\nselected features and the best split among those is chosen. When\n\nmax_features\n is set 1, this amounts to building a totally random\ndecision tree.\n\n\nWarning: Extra-trees should only be used within ensemble methods.\n\n\nRead more in the :ref:\nUser Guide <tree>\n.\n\n\nSee also\n\nExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n\n\nReferences\n\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\n\nMethods\n\n\nExtraTreeQuantileRegressor.predict(X, quantile=None, check_input=False)\n\n\nPredict regression value for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\nquantile\n (int, optional)\n\n\nValue ranging from 0 to 100. By default, the mean is returned.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples])\n\n\nIf quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.\n\n\n\n\n\n\nProperties\n\n\nskgarden.quantile.ExtraTreesQuantileRegressor\n\n\nAn extra-trees regressor that provides quantile estimates.\n\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and use averaging to improve the predictive accuracy\nand control over-fitting.\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\ncriterion\n (string, optional (default=\"mse\"))\n\n\nThe function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.\n\n\n\n\n\n\nmax_features\n (int, float, string or None, optional (default=\"auto\"))\n\n\nThe number of features to consider when looking for the best split:\n- If int, then consider \nmax_features\n features at each split.\n- If float, then \nmax_features\n is a percentage and\n  \nint(max_features * n_features)\n features are considered at each\n  split.\n- If \"auto\", then \nmax_features=n_features\n.\n- If \"sqrt\", then \nmax_features=sqrt(n_features)\n.\n- If \"log2\", then \nmax_features=log2(n_features)\n.\n- If None, then \nmax_features=n_features\n.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than \nmax_features\n features.\n\n\n\n\n\n\nmax_depth\n (integer or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider \nmin_samples_split\n as the minimum number.\n- If float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_samples_leaf\n (int, float, optional (default=1))\n\n\nThe minimum number of samples required to be at a leaf node:\n- If int, then consider \nmin_samples_leaf\n as the minimum number.\n- If float, then \nmin_samples_leaf\n is a percentage and\n  \nceil(min_samples_leaf * n_samples)\n are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_weight_fraction_leaf\n (float, optional (default=0.))\n\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n\n\n\n\n\n\nmax_leaf_nodes\n (int or None, optional (default=None))\n\n\nGrow trees with \nmax_leaf_nodes\n in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=False))\n\n\nWhether bootstrap samples are used when building trees.\n\n\n\n\n\n\noob_score\n (bool, optional (default=False))\n\n\nWhether to use out-of-bag samples to estimate the R^2 on unseen data.\n\n\n\n\n\n\nn_jobs\n (integer, optional (default=1))\n\n\nThe number of jobs to run in parallel for both \nfit\n and \npredict\n.\nIf -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nverbose\n (int, optional (default=0))\n\n\nControls the verbosity of the tree building process.\n\n\n\n\n\n\nwarm_start\n (bool, optional (default=False))\n\n\nWhen set to \nTrue\n, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nestimators_\n (list of ExtraTreeQuantileRegressor)\n\n\nThe collection of fitted sub-estimators.\n\n\n\n\n\n\nfeature_importances_\n (array of shape = [n_features])\n\n\nThe feature importances (the higher, the more important the feature).\n\n\n\n\n\n\nn_features_\n (int)\n\n\nThe number of features when \nfit\n is performed.\n\n\n\n\n\n\nn_outputs_\n (int)\n\n\nThe number of outputs when \nfit\n is performed.\n\n\n\n\n\n\noob_score_\n (float)\n\n\nScore of the training dataset obtained using an out-of-bag estimate.\n\n\n\n\n\n\noob_prediction_\n (array of shape = [n_samples])\n\n\nPrediction computed with out-of-bag estimate on the training set.\n\n\n\n\n\n\ny_train_\n (array-like, shape=(n_samples,))\n\n\nCache the target values at fit time.\n\n\n\n\n\n\ny_weights_\n (array-like, shape=(n_estimators, n_samples))\n\n\ny_weights_[i, j] is the weight given to sample \nj` while\nestimator\ni`` is fit. If bootstrap is set to True, this\nreduces to a 2-D array of ones.\n\n\n\n\n\n\ny_train_leaves_\n (array-like, shape=(n_estimators, n_samples))\n\n\ny_train_leaves_[i, j] provides the leaf node that y_train_[i]\nends up when estimator j is fit. If y_train_[i] is given\na weight of zero when estimator j is fit, then the value is -1.\n\n\n\n\n\n\nReferences\n\n.. [1] Nicolai Meinshausen, Quantile Regression Forests\n    http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf\n\n\nMethods\n\n\nExtraTreesQuantileRegressor.fit(X, y)\n\n\nBuild a forest from the training set (X, y).\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe training input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsc_matrix\n.\n\n\n\n\n\n\ny\n (array-like, shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe target values (class labels) as integers or strings.\n\n\n\n\n\n\nsample_weight\n (array-like, shape = [n_samples] or None)\n\n\nSample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nX_idx_sorted\n (array-like, shape = [n_samples, n_features], optional)\n\n\nThe indexes of the sorted training input samples. If many tree\nare grown on the same dataset, this allows the ordering to be\ncached between trees. If None, the data will be sorted here.\nDon't use this parameter unless you know what to do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nself\n (object)\n\n\nReturns self.\n\n\n\n\n\n\nExtraTreesQuantileRegressor.predict(X, quantile=None)\n\n\nPredict regression value for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\nquantile\n (int, optional)\n\n\nValue ranging from 0 to 100. By default, the mean is returned.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples])\n\n\nIf quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.\n\n\n\n\n\n\nProperties\n\n\nskgarden.quantile.RandomForestQuantileRegressor\n\n\nA random forest regressor that provides quantile estimates.\n\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and use averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if\n\nbootstrap=True\n (default).\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\ncriterion\n (string, optional (default=\"mse\"))\n\n\nThe function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.\n\n\n\n\n\n\nmax_features\n (int, float, string or None, optional (default=\"auto\"))\n\n\nThe number of features to consider when looking for the best split:\n- If int, then consider \nmax_features\n features at each split.\n- If float, then \nmax_features\n is a percentage and\n  \nint(max_features * n_features)\n features are considered at each\n  split.\n- If \"auto\", then \nmax_features=n_features\n.\n- If \"sqrt\", then \nmax_features=sqrt(n_features)\n.\n- If \"log2\", then \nmax_features=log2(n_features)\n.\n- If None, then \nmax_features=n_features\n.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than \nmax_features\n features.\n\n\n\n\n\n\nmax_depth\n (integer or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider \nmin_samples_split\n as the minimum number.\n- If float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_samples_leaf\n (int, float, optional (default=1))\n\n\nThe minimum number of samples required to be at a leaf node:\n- If int, then consider \nmin_samples_leaf\n as the minimum number.\n- If float, then \nmin_samples_leaf\n is a percentage and\n  \nceil(min_samples_leaf * n_samples)\n are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.\n\n\n\n\n\n\nmin_weight_fraction_leaf\n (float, optional (default=0.))\n\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n\n\n\n\n\n\nmax_leaf_nodes\n (int or None, optional (default=None))\n\n\nGrow trees with \nmax_leaf_nodes\n in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=True))\n\n\nWhether bootstrap samples are used when building trees.\n\n\n\n\n\n\noob_score\n (bool, optional (default=False))\n\n\nwhether to use out-of-bag samples to estimate\nthe R^2 on unseen data.\n\n\n\n\n\n\nn_jobs\n (integer, optional (default=1))\n\n\nThe number of jobs to run in parallel for both \nfit\n and \npredict\n.\nIf -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nverbose\n (int, optional (default=0))\n\n\nControls the verbosity of the tree building process.\n\n\n\n\n\n\nwarm_start\n (bool, optional (default=False))\n\n\nWhen set to \nTrue\n, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nestimators_\n (list of DecisionTreeQuantileRegressor)\n\n\nThe collection of fitted sub-estimators.\n\n\n\n\n\n\nfeature_importances_\n (array of shape = [n_features])\n\n\nThe feature importances (the higher, the more important the feature).\n\n\n\n\n\n\nn_features_\n (int)\n\n\nThe number of features when \nfit\n is performed.\n\n\n\n\n\n\nn_outputs_\n (int)\n\n\nThe number of outputs when \nfit\n is performed.\n\n\n\n\n\n\noob_score_\n (float)\n\n\nScore of the training dataset obtained using an out-of-bag estimate.\n\n\n\n\n\n\noob_prediction_\n (array of shape = [n_samples])\n\n\nPrediction computed with out-of-bag estimate on the training set.\n\n\n\n\n\n\ny_train_\n (array-like, shape=(n_samples,))\n\n\nCache the target values at fit time.\n\n\n\n\n\n\ny_weights_\n (array-like, shape=(n_estimators, n_samples))\n\n\ny_weights_[i, j] is the weight given to sample \nj` while\nestimator\ni`` is fit. If bootstrap is set to True, this\nreduces to a 2-D array of ones.\n\n\n\n\n\n\ny_train_leaves_\n (array-like, shape=(n_estimators, n_samples))\n\n\ny_train_leaves_[i, j] provides the leaf node that y_train_[i]\nends up when estimator j is fit. If y_train_[i] is given\na weight of zero when estimator j is fit, then the value is -1.\n\n\n\n\n\n\nReferences\n\n.. [1] Nicolai Meinshausen, Quantile Regression Forests\n    http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf\n\n\nMethods\n\n\nRandomForestQuantileRegressor.fit(X, y)\n\n\nBuild a forest from the training set (X, y).\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix, shape = [n_samples, n_features])\n\n\nThe training input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsc_matrix\n.\n\n\n\n\n\n\ny\n (array-like, shape = [n_samples] or [n_samples, n_outputs])\n\n\nThe target values (class labels) as integers or strings.\n\n\n\n\n\n\nsample_weight\n (array-like, shape = [n_samples] or None)\n\n\nSample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nX_idx_sorted\n (array-like, shape = [n_samples, n_features], optional)\n\n\nThe indexes of the sorted training input samples. If many tree\nare grown on the same dataset, this allows the ordering to be\ncached between trees. If None, the data will be sorted here.\nDon't use this parameter unless you know what to do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\nself\n (object)\n\n\nReturns self.\n\n\n\n\n\n\nRandomForestQuantileRegressor.predict(X, quantile=None)\n\n\nPredict regression value for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like or sparse matrix of shape = [n_samples, n_features])\n\n\nThe input samples. Internally, it will be converted to\n\ndtype=np.float32\n and if a sparse matrix is provided\nto a sparse \ncsr_matrix\n.\n\n\n\n\n\n\nquantile\n (int, optional)\n\n\nValue ranging from 0 to 100. By default, the mean is returned.\n\n\n\n\n\n\ncheck_input\n (boolean, (default=True))\n\n\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\ny\n (array of shape = [n_samples])\n\n\nIf quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.\n\n\n\n\n\n\nProperties\n\n\nskgarden.forest\n\n\nskgarden.forest.ExtraTreesRegressor\n\n\nExtraTreesRegressor that supports conditional standard deviation.\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\ncriterion\n (string, optional (default=\"mse\"))\n\n\nThe function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n\n\n\n\n\n\nmax_features\n (int, float, string or None, optional (default=\"auto\"))\n\n\nThe number of features to consider when looking for the best split:\n- If int, then consider \nmax_features\n features at each split.\n- If float, then \nmax_features\n is a percentage and\n  \nint(max_features * n_features)\n features are considered at each\n  split.\n- If \"auto\", then \nmax_features=n_features\n.\n- If \"sqrt\", then \nmax_features=sqrt(n_features)\n.\n- If \"log2\", then \nmax_features=log2(n_features)\n.\n- If None, then \nmax_features=n_features\n.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than \nmax_features\n features.\n\n\n\n\n\n\nmax_depth\n (integer or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider \nmin_samples_split\n as the minimum number.\n- If float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n\n\n\n\n\n\nmin_samples_leaf\n (int, float, optional (default=1))\n\n\nThe minimum number of samples required to be at a leaf node:\n- If int, then consider \nmin_samples_leaf\n as the minimum number.\n- If float, then \nmin_samples_leaf\n is a percentage and\n  \nceil(min_samples_leaf * n_samples)\n are the minimum\n  number of samples for each node.\n\n\n\n\n\n\nmin_weight_fraction_leaf\n (float, optional (default=0.))\n\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n\n\n\n\n\n\nmax_leaf_nodes\n (int or None, optional (default=None))\n\n\nGrow trees with \nmax_leaf_nodes\n in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n\n\n\n\n\n\nmin_impurity_decrease\n (float, optional (default=0.))\n\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following::\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\nwhere \nN\n is the total number of samples, \nN_t\n is the number of\nsamples at the current node, \nN_t_L\n is the number of samples in the\nleft child, and \nN_t_R\n is the number of samples in the right child.\n\nN\n, \nN_t\n, \nN_t_R\n and \nN_t_L\n all refer to the weighted sum,\nif \nsample_weight\n is passed.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=True))\n\n\nWhether bootstrap samples are used when building trees.\n\n\n\n\n\n\noob_score\n (bool, optional (default=False))\n\n\nwhether to use out-of-bag samples to estimate\nthe R^2 on unseen data.\n\n\n\n\n\n\nn_jobs\n (integer, optional (default=1))\n\n\nThe number of jobs to run in parallel for both \nfit\n and \npredict\n.\nIf -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nverbose\n (int, optional (default=0))\n\n\nControls the verbosity of the tree building process.\n\n\n\n\n\n\nwarm_start\n (bool, optional (default=False))\n\n\nWhen set to \nTrue\n, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nestimators_\n (list of DecisionTreeRegressor)\n\n\nThe collection of fitted sub-estimators.\n\n\n\n\n\n\nfeature_importances_\n (array of shape = [n_features])\n\n\nThe feature importances (the higher, the more important the feature).\n\n\n\n\n\n\nn_features_\n (int)\n\n\nThe number of features when \nfit\n is performed.\n\n\n\n\n\n\nn_outputs_\n (int)\n\n\nThe number of outputs when \nfit\n is performed.\n\n\n\n\n\n\noob_score_\n (float)\n\n\nScore of the training dataset obtained using an out-of-bag estimate.\n\n\n\n\n\n\noob_prediction_\n (array of shape = [n_samples])\n\n\nPrediction computed with out-of-bag estimate on the training set.\n\n\n\n\n\n\nNotes\n\nThe default values for the parameters controlling the size of the trees\n(e.g. \nmax_depth\n, \nmin_samples_leaf\n, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n\nmax_features=n_features\n and \nbootstrap=False\n, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, \nrandom_state\n has to be fixed.\n\n\nReferences\n\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n\nMethods\n\n\nExtraTreesRegressor.predict(X, return_std=False)\n\n\nPredict continuous output for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array-like of shape=(n_samples, n_features))\n\n\nInput data.\n\n\n\n\n\n\nreturn_std\n (boolean)\n\n\nWhether or not to return the standard deviation.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\npredictions\n (array-like of shape=(n_samples,))\n\n\nPredicted values for X. If criterion is set to \"mse\",\nthen \npredictions[i] ~= mean(y | X[i])\n.\n\n\n\n\n\n\nstd\n (array-like of shape=(n_samples,))\n\n\nStandard deviation of \ny\n at \nX\n. If criterion\nis set to \"mse\", then \nstd[i] ~= std(y | X[i])\n.\n\n\n\n\n\n\nProperties\n\n\nskgarden.forest.RandomForestRegressor\n\n\nRandomForestRegressor that supports conditional std computation.\n\n\nParameters\n\n\n\n\n\n\nn_estimators\n (integer, optional (default=10))\n\n\nThe number of trees in the forest.\n\n\n\n\n\n\ncriterion\n (string, optional (default=\"mse\"))\n\n\nThe function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n\n\n\n\n\n\nmax_features\n (int, float, string or None, optional (default=\"auto\"))\n\n\nThe number of features to consider when looking for the best split:\n- If int, then consider \nmax_features\n features at each split.\n- If float, then \nmax_features\n is a percentage and\n  \nint(max_features * n_features)\n features are considered at each\n  split.\n- If \"auto\", then \nmax_features=n_features\n.\n- If \"sqrt\", then \nmax_features=sqrt(n_features)\n.\n- If \"log2\", then \nmax_features=log2(n_features)\n.\n- If None, then \nmax_features=n_features\n.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than \nmax_features\n features.\n\n\n\n\n\n\nmax_depth\n (integer or None, optional (default=None))\n\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n\n\n\n\n\n\nmin_samples_split\n (int, float, optional (default=2))\n\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider \nmin_samples_split\n as the minimum number.\n- If float, then \nmin_samples_split\n is a percentage and\n  \nceil(min_samples_split * n_samples)\n are the minimum\n  number of samples for each split.\n\n\n\n\n\n\nmin_samples_leaf\n (int, float, optional (default=1))\n\n\nThe minimum number of samples required to be at a leaf node:\n- If int, then consider \nmin_samples_leaf\n as the minimum number.\n- If float, then \nmin_samples_leaf\n is a percentage and\n  \nceil(min_samples_leaf * n_samples)\n are the minimum\n  number of samples for each node.\n\n\n\n\n\n\nmin_weight_fraction_leaf\n (float, optional (default=0.))\n\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n\n\n\n\n\n\nmax_leaf_nodes\n (int or None, optional (default=None))\n\n\nGrow trees with \nmax_leaf_nodes\n in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n\n\n\n\n\n\nmin_impurity_decrease\n (float, optional (default=0.))\n\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following::\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\nwhere \nN\n is the total number of samples, \nN_t\n is the number of\nsamples at the current node, \nN_t_L\n is the number of samples in the\nleft child, and \nN_t_R\n is the number of samples in the right child.\n\nN\n, \nN_t\n, \nN_t_R\n and \nN_t_L\n all refer to the weighted sum,\nif \nsample_weight\n is passed.\n\n\n\n\n\n\nbootstrap\n (boolean, optional (default=True))\n\n\nWhether bootstrap samples are used when building trees.\n\n\n\n\n\n\noob_score\n (bool, optional (default=False))\n\n\nwhether to use out-of-bag samples to estimate\nthe R^2 on unseen data.\n\n\n\n\n\n\nn_jobs\n (integer, optional (default=1))\n\n\nThe number of jobs to run in parallel for both \nfit\n and \npredict\n.\nIf -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\nrandom_state\n (int, RandomState instance or None, optional (default=None))\n\n\nIf int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby \nnp.random\n.\n\n\n\n\n\n\nverbose\n (int, optional (default=0))\n\n\nControls the verbosity of the tree building process.\n\n\n\n\n\n\nwarm_start\n (bool, optional (default=False))\n\n\nWhen set to \nTrue\n, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.\n\n\n\n\n\n\nAttributes\n\n\n\n\n\n\nestimators_\n (list of DecisionTreeRegressor)\n\n\nThe collection of fitted sub-estimators.\n\n\n\n\n\n\nfeature_importances_\n (array of shape = [n_features])\n\n\nThe feature importances (the higher, the more important the feature).\n\n\n\n\n\n\nn_features_\n (int)\n\n\nThe number of features when \nfit\n is performed.\n\n\n\n\n\n\nn_outputs_\n (int)\n\n\nThe number of outputs when \nfit\n is performed.\n\n\n\n\n\n\noob_score_\n (float)\n\n\nScore of the training dataset obtained using an out-of-bag estimate.\n\n\n\n\n\n\noob_prediction_\n (array of shape = [n_samples])\n\n\nPrediction computed with out-of-bag estimate on the training set.\n\n\n\n\n\n\nNotes\n\nThe default values for the parameters controlling the size of the trees\n(e.g. \nmax_depth\n, \nmin_samples_leaf\n, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n\nmax_features=n_features\n and \nbootstrap=False\n, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, \nrandom_state\n has to be fixed.\n\n\nReferences\n\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n\nMethods\n\n\nRandomForestRegressor.predict(X, return_std=False)\n\n\nPredict continuous output for X.\n\n\nParameters\n\n\n\n\n\n\nX\n (array of shape = (n_samples, n_features))\n\n\nInput data.\n\n\n\n\n\n\nreturn_std\n (boolean)\n\n\nWhether or not to return the standard deviation.\n\n\n\n\n\n\nReturns\n\n\n\n\n\n\npredictions\n (array-like of shape = (n_samples,))\n\n\nPredicted values for X. If criterion is set to \"mse\",\nthen \npredictions[i] ~= mean(y | X[i])\n.\n\n\n\n\n\n\nstd\n (array-like of shape=(n_samples,))\n\n\nStandard deviation of \ny\n at \nX\n. If criterion\nis set to \"mse\", then \nstd[i] ~= std(y | X[i])\n.\n\n\n\n\n\n\nProperties",
            "title": "API Reference"
        },
        {
            "location": "/api/#api-documentation-of-skgarden",
            "text": "",
            "title": "API documentation of skgarden"
        },
        {
            "location": "/api/#table-of-contents",
            "text": "",
            "title": "Table of contents"
        },
        {
            "location": "/api/#skgardenmondrian",
            "text": "skgarden.mondrian.MondrianForestClassifier  skgarden.mondrian.MondrianForestRegressor  skgarden.mondrian.MondrianTreeClassifier  skgarden.mondrian.MondrianTreeRegressor",
            "title": "skgarden.mondrian"
        },
        {
            "location": "/api/#skgardenquantile",
            "text": "skgarden.quantile.DecisionTreeQuantileRegressor  skgarden.quantile.ExtraTreeQuantileRegressor  skgarden.quantile.ExtraTreesQuantileRegressor  skgarden.quantile.RandomForestQuantileRegressor",
            "title": "skgarden.quantile"
        },
        {
            "location": "/api/#skgardenforest",
            "text": "skgarden.forest.ExtraTreesRegressor  skgarden.forest.RandomForestRegressor",
            "title": "skgarden.forest"
        },
        {
            "location": "/api/#skgardenmondrian_1",
            "text": "",
            "title": "skgarden.mondrian"
        },
        {
            "location": "/api/#skgardenmondrianmondrianforestclassifier",
            "text": "A MondrianForestClassifier is an ensemble of MondrianTreeClassifiers.  The probability  p_{j}  of class  j  is given \\sum_{i}^{N_{est}} \\frac{p_{j}^i}{N_{est}}   Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    max_depth  (integer, optional (default=None))  The depth to which each tree is grown. If None, the tree is either\ngrown to full depth or is constrained by  min_samples_split .    min_samples_split  (integer, optional (default=2))  Stop growing the tree if all the nodes have lesser than min_samples_split  number of samples.    bootstrap  (boolean, optional (default=False))  If bootstrap is set to False, then all trees are trained on the\nentire training dataset. Else, each tree is fit on n_samples\ndrawn with replacement from the training dataset.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .",
            "title": "skgarden.mondrian.MondrianForestClassifier"
        },
        {
            "location": "/api/#methods",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#mondrianforestclassifierfitx-y",
            "text": "Builds a forest of trees from the training set (X, y).  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The training input samples. Internally, its dtype will be converted\nto  dtype=np.float32 . If a sparse matrix is provided, it will be\nconverted into a sparse  csc_matrix .    y  (array-like, shape = [n_samples] or [n_samples, n_outputs])  The target values (class labels in classification, real numbers in\nregression).    sample_weight  (array-like, shape = [n_samples] or None)  Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node.    Returns    self  (object)  Returns self.",
            "title": "MondrianForestClassifier.fit(X, y)"
        },
        {
            "location": "/api/#mondrianforestclassifierweighted_decision_pathx",
            "text": "Returns the weighted decision path in the forest.  Each non-zero value in the decision path determines the\nweight of that particular node while making predictions.  Parameters    X  (array-like, shape = (n_samples, n_features))  Input.    Returns    decision_path  (sparse csr matrix, shape = (n_samples, n_total_nodes))  Return a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.    est_inds  (array-like, shape = (n_estimators + 1,))  weighted_decision_path[:, est_inds[i]: est_inds[i + 1]]\nprovides the weighted_decision_path of estimator i",
            "title": "MondrianForestClassifier.weighted_decision_path(X)"
        },
        {
            "location": "/api/#properties",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenmondrianmondrianforestregressor",
            "text": "A MondrianForestRegressor is an ensemble of MondrianTreeRegressors.  The variance in predictions is reduced by averaging the predictions\nfrom all trees.  Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    max_depth  (integer, optional (default=None))  The depth to which each tree is grown. If None, the tree is either\ngrown to full depth or is constrained by  min_samples_split .    min_samples_split  (integer, optional (default=2))  Stop growing the tree if all the nodes have lesser than min_samples_split  number of samples.    bootstrap  (boolean, optional (default=False))  If bootstrap is set to False, then all trees are trained on the\nentire training dataset. Else, each tree is fit on n_samples\ndrawn with replacement from the training dataset.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .",
            "title": "skgarden.mondrian.MondrianForestRegressor"
        },
        {
            "location": "/api/#methods_1",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#mondrianforestregressorfitx-y",
            "text": "Builds a forest of trees from the training set (X, y).  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The training input samples. Internally, its dtype will be converted\nto  dtype=np.float32 . If a sparse matrix is provided, it will be\nconverted into a sparse  csc_matrix .    y  (array-like, shape = [n_samples] or [n_samples, n_outputs])  The target values (class labels in classification, real numbers in\nregression).    sample_weight  (array-like, shape = [n_samples] or None)  Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node.    Returns    self  (object)  Returns self.",
            "title": "MondrianForestRegressor.fit(X, y)"
        },
        {
            "location": "/api/#mondrianforestregressorpredictx-return_stdfalse",
            "text": "Returns the predicted mean and std.  The prediction is a GMM drawn from \\sum_{i=1}^T w_i N(m_i, \\sigma_i)  where  w_i = {1 \\over T} .  The mean  E[Y | X]  reduces to  {\\sum_{i=1}^T m_i \\over T}   The variance  Var[Y | X]  is given by  Var[Y | X] = E[Y^2 | X] - E[Y | X]^2  =\\frac{\\sum_{i=1}^T E[Y^2_i| X]}{T} - E[Y | X]^2  = \\frac{\\sum_{i=1}^T (Var[Y_i | X] + E[Y_i | X]^2)}{T} - E[Y| X]^2   Parameters    X  (array-like, shape = (n_samples, n_features))  Input samples.    return_std  (boolean, default (False))  Whether or not to return the standard deviation.    Returns    y  (array-like, shape = (n_samples,))  Predictions at X.    std  (array-like, shape = (n_samples,))  Standard deviation at X.",
            "title": "MondrianForestRegressor.predict(X, return_std=False)"
        },
        {
            "location": "/api/#mondrianforestregressorweighted_decision_pathx",
            "text": "Returns the weighted decision path in the forest.  Each non-zero value in the decision path determines the\nweight of that particular node while making predictions.  Parameters    X  (array-like, shape = (n_samples, n_features))  Input.    Returns    decision_path  (sparse csr matrix, shape = (n_samples, n_total_nodes))  Return a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.    est_inds  (array-like, shape = (n_estimators + 1,))  weighted_decision_path[:, est_inds[i]: est_inds[i + 1]]\nprovides the weighted_decision_path of estimator i",
            "title": "MondrianForestRegressor.weighted_decision_path(X)"
        },
        {
            "location": "/api/#properties_1",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenmondrianmondriantreeclassifier",
            "text": "A Mondrian tree.  The splits in a mondrian tree regressor differ from the standard regression\ntree in the following ways.  At fit time:\n    - Splits are done independently of the labels.\n    - The candidate feature is drawn with a probability proportional to the\n      feature range.\n    - The candidate threshold is drawn from a uniform distribution\n      with the bounds equal to the bounds of the candidate feature.\n    - The time of split is also stored which is proportional to the\n      inverse of the size of the bounding-box.  At prediction time:\n    - Every node in the path from the root to the leaf is given a weight\n      while making predictions.\n    - At each node, the probability of an unseen sample splitting from that\n      node is calculated. The farther the sample is away from the bounding\n      box, the more probable that it will split away.\n    - For every node, the probability that an unseen sample has not split\n      before reaching that node and the probability that it will split away\n      at that particular node are multiplied to give a weight.  Parameters    max_depth  (int or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:   If int, then consider  min_samples_split  as the minimum number.  If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.     random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .",
            "title": "skgarden.mondrian.MondrianTreeClassifier"
        },
        {
            "location": "/api/#methods_2",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#mondriantreeclassifierapplyx-check_inputtrue",
            "text": "Returns the index of the leaf that each sample is predicted as.  .. versionadded:: 0.17  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    X_leaves  (array_like, shape = [n_samples,])  For each datapoint x in X, return the index of the leaf x\nends up in. Leaves are numbered within [0; self.tree_.node_count) , possibly with gaps in the\nnumbering.",
            "title": "MondrianTreeClassifier.apply(X, check_input=True)"
        },
        {
            "location": "/api/#mondriantreeclassifierdecision_pathx-check_inputtrue",
            "text": "Return the decision path in the tree  .. versionadded:: 0.18  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    indicator  (sparse csr array, shape = [n_samples, n_nodes])  Return a node indicator matrix where non zero elements\nindicates that the samples goes through the nodes.",
            "title": "MondrianTreeClassifier.decision_path(X, check_input=True)"
        },
        {
            "location": "/api/#mondriantreeclassifierfitx-y-sample_weightnone-check_inputtrue-x_idx_sortednone",
            "text": "",
            "title": "MondrianTreeClassifier.fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)"
        },
        {
            "location": "/api/#mondriantreeclassifierpredictx-check_inputtrue-return_stdfalse",
            "text": "Predict class or regression value for X.  For a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    return_std  (boolean, (default=True))  Whether or not to return the standard deviation.    Returns    y  (array of shape = [n_samples] or [n_samples, n_outputs])  The predicted classes, or the predict values.",
            "title": "MondrianTreeClassifier.predict(X, check_input=True, return_std=False)"
        },
        {
            "location": "/api/#mondriantreeclassifierpredict_probax-check_inputtrue",
            "text": "Predicts the probability of each class label given X.  Parameters    X  (array-like, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32 .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    y_prob  (array of shape = [n_samples, n_classes])  Prediceted probabilities for each class.",
            "title": "MondrianTreeClassifier.predict_proba(X, check_input=True)"
        },
        {
            "location": "/api/#mondriantreeclassifierweighted_decision_pathx-check_inputtrue",
            "text": "Returns the weighted decision path in the tree.  Each non-zero value in the decision path determines the weight\nof that particular node in making predictions.  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    indicator  (sparse csr array, shape = [n_samples, n_nodes])  Return a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.",
            "title": "MondrianTreeClassifier.weighted_decision_path(X, check_input=True)"
        },
        {
            "location": "/api/#properties_2",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenmondrianmondriantreeregressor",
            "text": "A Mondrian tree.  The splits in a mondrian tree regressor differ from the standard regression\ntree in the following ways.  At fit time:\n    - Splits are done independently of the labels.\n    - The candidate feature is drawn with a probability proportional to the\n      feature range.\n    - The candidate threshold is drawn from a uniform distribution\n      with the bounds equal to the bounds of the candidate feature.\n    - The time of split is also stored which is proportional to the\n      inverse of the size of the bounding-box.  At prediction time:\n    - Every node in the path from the root to the leaf is given a weight\n      while making predictions.\n    - At each node, the probability of an unseen sample splitting from that\n      node is calculated. The farther the sample is away from the bounding\n      box, the more probable that it will split away.\n    - For every node, the probability that an unseen sample has not split\n      before reaching that node and the probability that it will split away\n      at that particular node are multiplied to give a weight.  Parameters    max_depth  (int or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:   If int, then consider  min_samples_split  as the minimum number.  If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.     random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .",
            "title": "skgarden.mondrian.MondrianTreeRegressor"
        },
        {
            "location": "/api/#methods_3",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#mondriantreeregressorapplyx-check_inputtrue",
            "text": "Returns the index of the leaf that each sample is predicted as.  .. versionadded:: 0.17  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    X_leaves  (array_like, shape = [n_samples,])  For each datapoint x in X, return the index of the leaf x\nends up in. Leaves are numbered within [0; self.tree_.node_count) , possibly with gaps in the\nnumbering.",
            "title": "MondrianTreeRegressor.apply(X, check_input=True)"
        },
        {
            "location": "/api/#mondriantreeregressordecision_pathx-check_inputtrue",
            "text": "Return the decision path in the tree  .. versionadded:: 0.18  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    indicator  (sparse csr array, shape = [n_samples, n_nodes])  Return a node indicator matrix where non zero elements\nindicates that the samples goes through the nodes.",
            "title": "MondrianTreeRegressor.decision_path(X, check_input=True)"
        },
        {
            "location": "/api/#mondriantreeregressorfitx-y-sample_weightnone-check_inputtrue-x_idx_sortednone",
            "text": "",
            "title": "MondrianTreeRegressor.fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)"
        },
        {
            "location": "/api/#mondriantreeregressorpredictx-check_inputtrue-return_stdfalse",
            "text": "Predict class or regression value for X.  For a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    return_std  (boolean, (default=True))  Whether or not to return the standard deviation.    Returns    y  (array of shape = [n_samples] or [n_samples, n_outputs])  The predicted classes, or the predict values.",
            "title": "MondrianTreeRegressor.predict(X, check_input=True, return_std=False)"
        },
        {
            "location": "/api/#mondriantreeregressorweighted_decision_pathx-check_inputtrue",
            "text": "Returns the weighted decision path in the tree.  Each non-zero value in the decision path determines the weight\nof that particular node in making predictions.  Parameters    X  (array_like or sparse matrix, shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    indicator  (sparse csr array, shape = [n_samples, n_nodes])  Return a node indicator matrix where non zero elements\nindicate the weight of that particular node in making predictions.",
            "title": "MondrianTreeRegressor.weighted_decision_path(X, check_input=True)"
        },
        {
            "location": "/api/#properties_3",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenquantile_1",
            "text": "",
            "title": "skgarden.quantile"
        },
        {
            "location": "/api/#skgardenquantiledecisiontreequantileregressor",
            "text": "A decision tree regressor that provides quantile estimates.  Parameters    criterion  (string, optional (default=\"mse\"))  The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.    splitter  (string, optional (default=\"best\"))  The strategy used to choose the split at each node. Supported\nstrategies are \"best\" to choose the best split and \"random\" to choose\nthe best random split.    max_features  (int, float, string or None, optional (default=None))  The number of features to consider when looking for the best split:\n- If int, then consider  max_features  features at each split.\n- If float, then  max_features  is a percentage and\n   int(max_features * n_features)  features are considered at each\n  split.\n- If \"auto\", then  max_features=n_features .\n- If \"sqrt\", then  max_features=sqrt(n_features) .\n- If \"log2\", then  max_features=log2(n_features) .\n- If None, then  max_features=n_features .\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than  max_features  features.    max_depth  (int or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:\n- If int, then consider  min_samples_split  as the minimum number.\n- If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_samples_leaf  (int, float, optional (default=1))  The minimum number of samples required to be at a leaf node:\n- If int, then consider  min_samples_leaf  as the minimum number.\n- If float, then  min_samples_leaf  is a percentage and\n   ceil(min_samples_leaf * n_samples)  are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_weight_fraction_leaf  (float, optional (default=0.))  The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.    max_leaf_nodes  (int or None, optional (default=None))  Grow a tree with  max_leaf_nodes  in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .    presort  (bool, optional (default=False))  Whether to presort the data to speed up the finding of best splits in\nfitting. For the default settings of a decision tree on large\ndatasets, setting this to true may slow down the training process.\nWhen using either a smaller dataset or a restricted depth, this may\nspeed up the training.    Attributes    feature_importances_  (array of shape = [n_features])  The feature importances.\nThe higher, the more important the feature.\nThe importance of a feature is computed as the\n(normalized) total reduction of the criterion brought\nby that feature. It is also known as the Gini importance [4]_.    max_features_  (int,)  The inferred value of max_features.    n_features_  (int)  The number of features when  fit  is performed.    n_outputs_  (int)  The number of outputs when  fit  is performed.    tree_  (Tree object)  The underlying Tree object.    y_train_  (array-like)  Train target values.    y_train_leaves_  (array-like.)  Cache the leaf nodes that each training sample falls into.\ny_train_leaves_[i] is the leaf that y_train[i] ends up at.",
            "title": "skgarden.quantile.DecisionTreeQuantileRegressor"
        },
        {
            "location": "/api/#methods_4",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#decisiontreequantileregressorpredictx-quantilenone-check_inputfalse",
            "text": "Predict regression value for X.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    quantile  (int, optional)  Value ranging from 0 to 100. By default, the mean is returned.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    y  (array of shape = [n_samples])  If quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.",
            "title": "DecisionTreeQuantileRegressor.predict(X, quantile=None, check_input=False)"
        },
        {
            "location": "/api/#properties_4",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenquantileextratreequantileregressor",
            "text": "An extremely randomized tree regressor.  Extra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the  max_features  randomly\nselected features and the best split among those is chosen. When max_features  is set 1, this amounts to building a totally random\ndecision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref: User Guide <tree> .  See also \nExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor  References  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.",
            "title": "skgarden.quantile.ExtraTreeQuantileRegressor"
        },
        {
            "location": "/api/#methods_5",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#extratreequantileregressorpredictx-quantilenone-check_inputfalse",
            "text": "Predict regression value for X.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    quantile  (int, optional)  Value ranging from 0 to 100. By default, the mean is returned.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    y  (array of shape = [n_samples])  If quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.",
            "title": "ExtraTreeQuantileRegressor.predict(X, quantile=None, check_input=False)"
        },
        {
            "location": "/api/#properties_5",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenquantileextratreesquantileregressor",
            "text": "An extra-trees regressor that provides quantile estimates.  This class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and use averaging to improve the predictive accuracy\nand control over-fitting.  Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    criterion  (string, optional (default=\"mse\"))  The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.    max_features  (int, float, string or None, optional (default=\"auto\"))  The number of features to consider when looking for the best split:\n- If int, then consider  max_features  features at each split.\n- If float, then  max_features  is a percentage and\n   int(max_features * n_features)  features are considered at each\n  split.\n- If \"auto\", then  max_features=n_features .\n- If \"sqrt\", then  max_features=sqrt(n_features) .\n- If \"log2\", then  max_features=log2(n_features) .\n- If None, then  max_features=n_features .\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than  max_features  features.    max_depth  (integer or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:\n- If int, then consider  min_samples_split  as the minimum number.\n- If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_samples_leaf  (int, float, optional (default=1))  The minimum number of samples required to be at a leaf node:\n- If int, then consider  min_samples_leaf  as the minimum number.\n- If float, then  min_samples_leaf  is a percentage and\n   ceil(min_samples_leaf * n_samples)  are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_weight_fraction_leaf  (float, optional (default=0.))  The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.    max_leaf_nodes  (int or None, optional (default=None))  Grow trees with  max_leaf_nodes  in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.    bootstrap  (boolean, optional (default=False))  Whether bootstrap samples are used when building trees.    oob_score  (bool, optional (default=False))  Whether to use out-of-bag samples to estimate the R^2 on unseen data.    n_jobs  (integer, optional (default=1))  The number of jobs to run in parallel for both  fit  and  predict .\nIf -1, then the number of jobs is set to the number of cores.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .    verbose  (int, optional (default=0))  Controls the verbosity of the tree building process.    warm_start  (bool, optional (default=False))  When set to  True , reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.    Attributes    estimators_  (list of ExtraTreeQuantileRegressor)  The collection of fitted sub-estimators.    feature_importances_  (array of shape = [n_features])  The feature importances (the higher, the more important the feature).    n_features_  (int)  The number of features when  fit  is performed.    n_outputs_  (int)  The number of outputs when  fit  is performed.    oob_score_  (float)  Score of the training dataset obtained using an out-of-bag estimate.    oob_prediction_  (array of shape = [n_samples])  Prediction computed with out-of-bag estimate on the training set.    y_train_  (array-like, shape=(n_samples,))  Cache the target values at fit time.    y_weights_  (array-like, shape=(n_estimators, n_samples))  y_weights_[i, j] is the weight given to sample  j` while\nestimator i`` is fit. If bootstrap is set to True, this\nreduces to a 2-D array of ones.    y_train_leaves_  (array-like, shape=(n_estimators, n_samples))  y_train_leaves_[i, j] provides the leaf node that y_train_[i]\nends up when estimator j is fit. If y_train_[i] is given\na weight of zero when estimator j is fit, then the value is -1.    References \n.. [1] Nicolai Meinshausen, Quantile Regression Forests\n    http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf",
            "title": "skgarden.quantile.ExtraTreesQuantileRegressor"
        },
        {
            "location": "/api/#methods_6",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#extratreesquantileregressorfitx-y",
            "text": "Build a forest from the training set (X, y).  Parameters    X  (array-like or sparse matrix, shape = [n_samples, n_features])  The training input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csc_matrix .    y  (array-like, shape = [n_samples] or [n_samples, n_outputs])  The target values (class labels) as integers or strings.    sample_weight  (array-like, shape = [n_samples] or None)  Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    X_idx_sorted  (array-like, shape = [n_samples, n_features], optional)  The indexes of the sorted training input samples. If many tree\nare grown on the same dataset, this allows the ordering to be\ncached between trees. If None, the data will be sorted here.\nDon't use this parameter unless you know what to do.    Returns    self  (object)  Returns self.",
            "title": "ExtraTreesQuantileRegressor.fit(X, y)"
        },
        {
            "location": "/api/#extratreesquantileregressorpredictx-quantilenone",
            "text": "Predict regression value for X.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    quantile  (int, optional)  Value ranging from 0 to 100. By default, the mean is returned.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    y  (array of shape = [n_samples])  If quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.",
            "title": "ExtraTreesQuantileRegressor.predict(X, quantile=None)"
        },
        {
            "location": "/api/#properties_6",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenquantilerandomforestquantileregressor",
            "text": "A random forest regressor that provides quantile estimates.  A random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and use averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if bootstrap=True  (default).  Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    criterion  (string, optional (default=\"mse\"))  The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion.    max_features  (int, float, string or None, optional (default=\"auto\"))  The number of features to consider when looking for the best split:\n- If int, then consider  max_features  features at each split.\n- If float, then  max_features  is a percentage and\n   int(max_features * n_features)  features are considered at each\n  split.\n- If \"auto\", then  max_features=n_features .\n- If \"sqrt\", then  max_features=sqrt(n_features) .\n- If \"log2\", then  max_features=log2(n_features) .\n- If None, then  max_features=n_features .\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than  max_features  features.    max_depth  (integer or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:\n- If int, then consider  min_samples_split  as the minimum number.\n- If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_samples_leaf  (int, float, optional (default=1))  The minimum number of samples required to be at a leaf node:\n- If int, then consider  min_samples_leaf  as the minimum number.\n- If float, then  min_samples_leaf  is a percentage and\n   ceil(min_samples_leaf * n_samples)  are the minimum\n  number of samples for each node.\n.. versionchanged:: 0.18\n   Added float values for percentages.    min_weight_fraction_leaf  (float, optional (default=0.))  The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.    max_leaf_nodes  (int or None, optional (default=None))  Grow trees with  max_leaf_nodes  in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.    bootstrap  (boolean, optional (default=True))  Whether bootstrap samples are used when building trees.    oob_score  (bool, optional (default=False))  whether to use out-of-bag samples to estimate\nthe R^2 on unseen data.    n_jobs  (integer, optional (default=1))  The number of jobs to run in parallel for both  fit  and  predict .\nIf -1, then the number of jobs is set to the number of cores.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .    verbose  (int, optional (default=0))  Controls the verbosity of the tree building process.    warm_start  (bool, optional (default=False))  When set to  True , reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.    Attributes    estimators_  (list of DecisionTreeQuantileRegressor)  The collection of fitted sub-estimators.    feature_importances_  (array of shape = [n_features])  The feature importances (the higher, the more important the feature).    n_features_  (int)  The number of features when  fit  is performed.    n_outputs_  (int)  The number of outputs when  fit  is performed.    oob_score_  (float)  Score of the training dataset obtained using an out-of-bag estimate.    oob_prediction_  (array of shape = [n_samples])  Prediction computed with out-of-bag estimate on the training set.    y_train_  (array-like, shape=(n_samples,))  Cache the target values at fit time.    y_weights_  (array-like, shape=(n_estimators, n_samples))  y_weights_[i, j] is the weight given to sample  j` while\nestimator i`` is fit. If bootstrap is set to True, this\nreduces to a 2-D array of ones.    y_train_leaves_  (array-like, shape=(n_estimators, n_samples))  y_train_leaves_[i, j] provides the leaf node that y_train_[i]\nends up when estimator j is fit. If y_train_[i] is given\na weight of zero when estimator j is fit, then the value is -1.    References \n.. [1] Nicolai Meinshausen, Quantile Regression Forests\n    http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf",
            "title": "skgarden.quantile.RandomForestQuantileRegressor"
        },
        {
            "location": "/api/#methods_7",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#randomforestquantileregressorfitx-y",
            "text": "Build a forest from the training set (X, y).  Parameters    X  (array-like or sparse matrix, shape = [n_samples, n_features])  The training input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csc_matrix .    y  (array-like, shape = [n_samples] or [n_samples, n_outputs])  The target values (class labels) as integers or strings.    sample_weight  (array-like, shape = [n_samples] or None)  Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. Splits are also\nignored if they would result in any single class carrying a\nnegative weight in either child node.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    X_idx_sorted  (array-like, shape = [n_samples, n_features], optional)  The indexes of the sorted training input samples. If many tree\nare grown on the same dataset, this allows the ordering to be\ncached between trees. If None, the data will be sorted here.\nDon't use this parameter unless you know what to do.    Returns    self  (object)  Returns self.",
            "title": "RandomForestQuantileRegressor.fit(X, y)"
        },
        {
            "location": "/api/#randomforestquantileregressorpredictx-quantilenone",
            "text": "Predict regression value for X.  Parameters    X  (array-like or sparse matrix of shape = [n_samples, n_features])  The input samples. Internally, it will be converted to dtype=np.float32  and if a sparse matrix is provided\nto a sparse  csr_matrix .    quantile  (int, optional)  Value ranging from 0 to 100. By default, the mean is returned.    check_input  (boolean, (default=True))  Allow to bypass several input checking.\nDon't use this parameter unless you know what you do.    Returns    y  (array of shape = [n_samples])  If quantile is set to None, then return E(Y | X). Else return\ny such that F(Y=y | x) = quantile.",
            "title": "RandomForestQuantileRegressor.predict(X, quantile=None)"
        },
        {
            "location": "/api/#properties_7",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenforest_1",
            "text": "",
            "title": "skgarden.forest"
        },
        {
            "location": "/api/#skgardenforestextratreesregressor",
            "text": "ExtraTreesRegressor that supports conditional standard deviation.  Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    criterion  (string, optional (default=\"mse\"))  The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.    max_features  (int, float, string or None, optional (default=\"auto\"))  The number of features to consider when looking for the best split:\n- If int, then consider  max_features  features at each split.\n- If float, then  max_features  is a percentage and\n   int(max_features * n_features)  features are considered at each\n  split.\n- If \"auto\", then  max_features=n_features .\n- If \"sqrt\", then  max_features=sqrt(n_features) .\n- If \"log2\", then  max_features=log2(n_features) .\n- If None, then  max_features=n_features .\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than  max_features  features.    max_depth  (integer or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:\n- If int, then consider  min_samples_split  as the minimum number.\n- If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.    min_samples_leaf  (int, float, optional (default=1))  The minimum number of samples required to be at a leaf node:\n- If int, then consider  min_samples_leaf  as the minimum number.\n- If float, then  min_samples_leaf  is a percentage and\n   ceil(min_samples_leaf * n_samples)  are the minimum\n  number of samples for each node.    min_weight_fraction_leaf  (float, optional (default=0.))  The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.    max_leaf_nodes  (int or None, optional (default=None))  Grow trees with  max_leaf_nodes  in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.    min_impurity_decrease  (float, optional (default=0.))  A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following::\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\nwhere  N  is the total number of samples,  N_t  is the number of\nsamples at the current node,  N_t_L  is the number of samples in the\nleft child, and  N_t_R  is the number of samples in the right child. N ,  N_t ,  N_t_R  and  N_t_L  all refer to the weighted sum,\nif  sample_weight  is passed.    bootstrap  (boolean, optional (default=True))  Whether bootstrap samples are used when building trees.    oob_score  (bool, optional (default=False))  whether to use out-of-bag samples to estimate\nthe R^2 on unseen data.    n_jobs  (integer, optional (default=1))  The number of jobs to run in parallel for both  fit  and  predict .\nIf -1, then the number of jobs is set to the number of cores.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .    verbose  (int, optional (default=0))  Controls the verbosity of the tree building process.    warm_start  (bool, optional (default=False))  When set to  True , reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.    Attributes    estimators_  (list of DecisionTreeRegressor)  The collection of fitted sub-estimators.    feature_importances_  (array of shape = [n_features])  The feature importances (the higher, the more important the feature).    n_features_  (int)  The number of features when  fit  is performed.    n_outputs_  (int)  The number of outputs when  fit  is performed.    oob_score_  (float)  Score of the training dataset obtained using an out-of-bag estimate.    oob_prediction_  (array of shape = [n_samples])  Prediction computed with out-of-bag estimate on the training set.    Notes \nThe default values for the parameters controlling the size of the trees\n(e.g.  max_depth ,  min_samples_leaf , etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data, max_features=n_features  and  bootstrap=False , if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting,  random_state  has to be fixed.  References \n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.",
            "title": "skgarden.forest.ExtraTreesRegressor"
        },
        {
            "location": "/api/#methods_8",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#extratreesregressorpredictx-return_stdfalse",
            "text": "Predict continuous output for X.  Parameters    X  (array-like of shape=(n_samples, n_features))  Input data.    return_std  (boolean)  Whether or not to return the standard deviation.    Returns    predictions  (array-like of shape=(n_samples,))  Predicted values for X. If criterion is set to \"mse\",\nthen  predictions[i] ~= mean(y | X[i]) .    std  (array-like of shape=(n_samples,))  Standard deviation of  y  at  X . If criterion\nis set to \"mse\", then  std[i] ~= std(y | X[i]) .",
            "title": "ExtraTreesRegressor.predict(X, return_std=False)"
        },
        {
            "location": "/api/#properties_8",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/api/#skgardenforestrandomforestregressor",
            "text": "RandomForestRegressor that supports conditional std computation.  Parameters    n_estimators  (integer, optional (default=10))  The number of trees in the forest.    criterion  (string, optional (default=\"mse\"))  The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.    max_features  (int, float, string or None, optional (default=\"auto\"))  The number of features to consider when looking for the best split:\n- If int, then consider  max_features  features at each split.\n- If float, then  max_features  is a percentage and\n   int(max_features * n_features)  features are considered at each\n  split.\n- If \"auto\", then  max_features=n_features .\n- If \"sqrt\", then  max_features=sqrt(n_features) .\n- If \"log2\", then  max_features=log2(n_features) .\n- If None, then  max_features=n_features .\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than  max_features  features.    max_depth  (integer or None, optional (default=None))  The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.    min_samples_split  (int, float, optional (default=2))  The minimum number of samples required to split an internal node:\n- If int, then consider  min_samples_split  as the minimum number.\n- If float, then  min_samples_split  is a percentage and\n   ceil(min_samples_split * n_samples)  are the minimum\n  number of samples for each split.    min_samples_leaf  (int, float, optional (default=1))  The minimum number of samples required to be at a leaf node:\n- If int, then consider  min_samples_leaf  as the minimum number.\n- If float, then  min_samples_leaf  is a percentage and\n   ceil(min_samples_leaf * n_samples)  are the minimum\n  number of samples for each node.    min_weight_fraction_leaf  (float, optional (default=0.))  The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.    max_leaf_nodes  (int or None, optional (default=None))  Grow trees with  max_leaf_nodes  in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.    min_impurity_decrease  (float, optional (default=0.))  A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following::\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\nwhere  N  is the total number of samples,  N_t  is the number of\nsamples at the current node,  N_t_L  is the number of samples in the\nleft child, and  N_t_R  is the number of samples in the right child. N ,  N_t ,  N_t_R  and  N_t_L  all refer to the weighted sum,\nif  sample_weight  is passed.    bootstrap  (boolean, optional (default=True))  Whether bootstrap samples are used when building trees.    oob_score  (bool, optional (default=False))  whether to use out-of-bag samples to estimate\nthe R^2 on unseen data.    n_jobs  (integer, optional (default=1))  The number of jobs to run in parallel for both  fit  and  predict .\nIf -1, then the number of jobs is set to the number of cores.    random_state  (int, RandomState instance or None, optional (default=None))  If int, random_state is the seed used by the random number generator;\nIf RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used\nby  np.random .    verbose  (int, optional (default=0))  Controls the verbosity of the tree building process.    warm_start  (bool, optional (default=False))  When set to  True , reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.    Attributes    estimators_  (list of DecisionTreeRegressor)  The collection of fitted sub-estimators.    feature_importances_  (array of shape = [n_features])  The feature importances (the higher, the more important the feature).    n_features_  (int)  The number of features when  fit  is performed.    n_outputs_  (int)  The number of outputs when  fit  is performed.    oob_score_  (float)  Score of the training dataset obtained using an out-of-bag estimate.    oob_prediction_  (array of shape = [n_samples])  Prediction computed with out-of-bag estimate on the training set.    Notes \nThe default values for the parameters controlling the size of the trees\n(e.g.  max_depth ,  min_samples_leaf , etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data, max_features=n_features  and  bootstrap=False , if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting,  random_state  has to be fixed.  References \n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.",
            "title": "skgarden.forest.RandomForestRegressor"
        },
        {
            "location": "/api/#methods_9",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/api/#randomforestregressorpredictx-return_stdfalse",
            "text": "Predict continuous output for X.  Parameters    X  (array of shape = (n_samples, n_features))  Input data.    return_std  (boolean)  Whether or not to return the standard deviation.    Returns    predictions  (array-like of shape = (n_samples,))  Predicted values for X. If criterion is set to \"mse\",\nthen  predictions[i] ~= mean(y | X[i]) .    std  (array-like of shape=(n_samples,))  Standard deviation of  y  at  X . If criterion\nis set to \"mse\", then  std[i] ~= std(y | X[i]) .",
            "title": "RandomForestRegressor.predict(X, return_std=False)"
        },
        {
            "location": "/api/#properties_9",
            "text": "",
            "title": "Properties"
        },
        {
            "location": "/examples/",
            "text": "Scikit-garden tutorials.\n\n\n\n\nIntuition behind Mondrian trees\n\n\nQuantile regression forests",
            "title": "Home"
        },
        {
            "location": "/examples/#scikit-garden-tutorials",
            "text": "Intuition behind Mondrian trees  Quantile regression forests",
            "title": "Scikit-garden tutorials."
        },
        {
            "location": "/examples/MondrianTreeRegressor/",
            "text": "Intuition behind Mondrian Trees\n\n\nThis example provides intuition behind Mondrian Trees. In particular, the differences between existing decision tree algorithms and explanations of the tree construction and prediction will be highlighted.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import ExtraTreeRegressor\nfrom skgarden import MondrianTreeRegressor\nfrom itertools import cycle\n%matplotlib inline\n\n\n\n\nA decision tree regressor\n\n\nA decision tree is one of the easier-to-understand machine learning algorithms. While training, the input training space \nX\n is recursively partitioned into a number of rectangular subspaces. While predicting the label of a new point, one determines the rectangular subspace that it falls into and outputs the label representative of that subspace. This is usually the mean of the labels for a regression problem.\n\n\nThe rectangular subspaces are constructed in a greedy manner doing a binary partition at a time. Hence this can be determined by a split \nS\n, a (f, \n\\delta)\n tuple where f is the feature index and \n\\delta\n is the threshold across which to split.\n\n\nA point \nx\n for which \nx[f]\n is lesser than \n\\delta\n is placed in the left subspace and vice versa.\n\n\nSo how is each split done?\n\n\nThe optimal split \nS_{opt}\n is determined by \n(f_{opt}, \\delta_{opt}\n) that creates children such that the weighted decrease in impurity is maximum. Mathematically, this is the combination that maximizes, \nC=N_{parent} Imp_{Parent} - N_{left} Imp_{left} - N_{right} Imp_{right}\n, where \nN_{parent}, N_{left}, N_{right}\n are the number of samples in the parent, left and right subspaces.\n\n\nIn a standard decision tree, \nS_{opt}\n is found out by searching though all possible combinations of feature indices and values in the training data and simply returning the combination that minimizes C as described above. That sounds pretty expensive!\n\n\nIn an extremely randomized tree this is made much faster by the following procedure.\n\n\n\n\nThe feature indices of the candidate splits are determined by drawing \nmax_features\n at random.\n\n\nThen for each of these feature index, we select the split threshold by drawing uniformly between the bounds of that feature.\n\n\n\n\nWhen the set of candidate splits are obtained, as before we just return that split that minimizes \nC\n.\n\n\nNote:\n\n\nIt is important to note that the actual reason is that while constructing an ensemble of trees, it makes sure that each tree constructed in an independent fashion. Decorrelating predictions in an ensemble is a key factor to achieve lower generalization error. For a highly unlikely corner case, if each tree in an ensemble is exactly the same, then there is no point constructing the ensemble.\n\n\nLet us now generate some toy data to play with it in the remainder of this example. Here toy data, meaning a set of ten points that lie on a sine curve.\n\n\ndef generate_toy_data(n_points=10):\n    X_train = np.linspace(-np.pi, np.pi, n_points)\n    y_train = np.sin(X_train)\n    X_test = np.linspace(-6.0, 6.0, 100)\n    y_test = np.sin(X_test)\n    return np.expand_dims(X_train, axis=1), y_train, np.expand_dims(X_test, axis=1), y_test\n\nX_train, y_train, X_test, y_test = generate_toy_data()\nplt.plot(X_train.ravel(), y_train, \"ro\")\nplt.show()\n\n\n\n\n\n\nPlotting decision boundaries using ERT's\n\n\nLet us now use scikit-learn's \nExtraTreeRegressor\n to train on the generated toy data, predict on some unseen data and plot decision boundaries in the 1-D space. Also, we set the \nmax_depth\n parameter to 2, which means there can be a maximum of 4 decision boundaries in the 1-D space.\n\n\ndef plot_1D_decision_boundaries(tree_reg, plt):\n    x_l, x_u = plt.xlim()\n    colors = cycle(\"bgrcmyk\")\n    split_thresh = tree_reg.tree_.threshold\n    split_thresh = sorted(split_thresh[split_thresh != -2])\n    split_thresh = [x_l] + split_thresh + [x_u]\n    for s in split_thresh:\n        plt.axvline(s, color=\"r\", linestyle=\"-\")\n\n    for x_l, x_u in zip(split_thresh[1:], split_thresh[:-1]):\n        plt.fill_between((x_l, x_u), -1.5, 1.5, alpha=0.2, color=next(colors))\n\netr = ExtraTreeRegressor(random_state=2, max_depth=2)\netr.fit(X_train, y_train)\ny_pred = etr.predict(X_test)\nplt.plot(X_train.ravel(), y_train, \"ro\")\nplt.plot(X_test.ravel(), y_pred)\nplt.ylim((-1.5, 1.5))\nplot_1D_decision_boundaries(etr, plt)\nplt.show()\n\n\n\n\n\n\nThe blue line represents the mean prediction in every region of the decision space. So if a point lies between -6 and -3, we predict y to be 0.0 and so on.\n\n\nThere are two things to notice.\n\n\n\n\nAs we move away from the training data, the predicted mean remains constant which is determined by the decision split at the bounds of the space. But in reality, we are unsure about the target value to predict and would like to fall back on some prior mean.\n\n\nIn an extremely randomized tree, this issue is not confined to subspaces at the edge of the training subspace. Even if the green subspace was not at the edge, we are unsure about the region from 4 to 6 since the train points are confined till 4.0\n\n\n\n\nThe mondrian tree solves this problem in a very intelligent way.\n\n\nMondrian Tree\n\n\nTrain mode\n\n\nThe split tuple (f, \n\\delta)\n is decided independently of the target or the decrease in impurity! Yes, that is right. When all the features are of same scale and are equally important, this is same as an extremely randomized tree with \nmax_features\n set to 1.\n\n\n\n\nThe split feature index \nf\n is drawn with a probability proportional to \nu_b[f] - l_b[f]\n where \nu_b\n and \nl_b\n and the upper and lower bounds of all the features.\n\n\nAfter fixing the feature index, the split threshold \n\\delta\n is then drawn from a uniform distribution with limits \nl_b\n, \nu_b\n.\n\n\n\n\nThe intuition being that a feature that has a huge difference between the bounds is likelier to be an \"important\" feature. Every subspace \nj\n in a mondrian tree also stores information about.\n\n\n\n\nThe upper and lower bounds of all the features in that particular node or the bounding box as determined by the training data. For example, in the green subspace above it stores ((1.96, 3.0),)\n\n\nThe time of split \n\\tau\n which is drawn from an exponential with mean \n\\sum_{f=1}^D(u_b[f] - l_b[f])\n. Smaller the value of tau, larger is the bounding box.\n\n\n\n\nNote:\n\n\nThe time of split can be viewed as weighted depth. Imagine that edges between parent and child nodes are associated with a non-negative weight. The time of split at a node is the sum of the weights along the path from the root to the node. If weights are all 1, the time of split is the depth of the node.\n\n\nPrediction mode\n\n\nFrom now on, we will use the terms node and subspace interchangeably. The subspace that a new point ends up in is the leaf, the entire training space is the root and every binary partition results in two nodes.\n\n\nRecall that for a decision tree, computing the prediction for a new point is fairly straightforward. Find the leaf node that a new point lands in and output the mean.\n\n\nThe prediction step of a Mondrian Tree is a bit more complicated. It takes into account all the nodes in the path of a new point from the root to the leaf for making a prediction. This formulation allows us the flexibility to weigh the nodes on the basis of how sure/unsure we are about the prediction in that particular node.\n\n\nMathematically, the distribution of \nP(Y | X)\n is given by\n\n\n\n\nP(Y | X) = \\sum_{j} w_j \\mathcal{N} (m_j, v_j)\n\n\n\n\nwhere the summation is across all the nodes in the path from the root to the leaf. The mean prediction becomes \n\\sum_{j} w_j m_j\n\n\n\n\nComputing the weights.\n\n\nAssume \np_j(x)\n denote the probability of a new point splitting away from a node. That is higher the probability, the farther away it is from the bounding box at that node.\n\n\nA point which is within the bounds at any node, the probability of separation should be zero. This means we can obtain a better estimate about the prediction from its child node.\n\n\nAlso, the node at which the new point starts to split away is the node that we are most confident about. So this should be given a high weight.\n\n\nFormally, the weights are computed like this.\n\n\nIf \nj\n is not a leaf:\n\n\n\n\nw_j(x) = p_j(x) \\prod_{k \\in anc(j)} (1 - p_k(x))\n\n\n\n\nIf \nj\n is a leaf, to make the weights sum up to one.\n\n\n\n\nw_j(x) = 1 - \\sum_{k \\in anc(j)} w_k(x)\n\n\n\n\n\n\nw_j\n can be decomposed into two factors, \np_j\n(x) and \n\\prod_{k \\in anc(j)} (1 - p_k(x))\n. The first one being the probability of splitting away at that particular node and the second one being the probability of not splitting away till it reaches that node. We can observe that for \nx\n that is completely within the bounds of a node, \nw_j(x)\n becomes zero and for a point where it starts branching off, \nw_j(x) = p_j(x)\n\n\n\n\nComputing the probability of separation.\n\n\nWe come to the final piece in the jigsaw puzzle, that is computing the probability of separation \np_j(x)\n of each node. This is computed in the following way.\n\n\n\n\n\n\n\\Delta_{j} = \\tau_{j} - \\tau_{parent(j)}\n\n\n\n\n\n\n\\eta_{j}(x) = \\sum_{f}(\\max(x[f] - u_{bj}[f], 0) + \\max(0, l_{bj}[f] - x[f]))\n\n\n\n\n\n\np_j(x) = 1 - e^{-\\Delta_{j} \\eta_{j}(x))}\n\n\n\n\n\n\nLet us take some time to stare at these equations and understand what they mean.\n\n\n\n\n\n\np_j(x)\n is high when \n\\eta_{j}(x)\n is high. As \n\\eta_{j}(x)\n approaches infinity, \np_j(x)\n approaches zero. This means that when the point is far away from the bounding box of the node, the probability of separation becomes high.\n\n\n\n\np_j(x)\n is high when \n\\Delta{j}\n is high. This means when the bounding box of a node is small as compared to the bounding box of its parent, the probability of separation becomes high.\n\n\nFor a point in the training data, \np_j(x)\n becomes zero for all nodes other than the leaf since the point is within the bounding box at all nodes. The leaf then has a weightage of 1.0 and this reduces to a standard decision tree prediction.\n\n\nFor a point far away from the training data, \np_{root}(x)\n (and \nw_{root}(x)\n) approach one and hence the weights of the other nodes in the path from the root to the leaf approach zero. This means \nP(Y | X) = \\mathcal{N}(m, v)\n where \nm\n and \nv\n are the empirical mean and variance of the training data.\n\n\n\n\nPlotting decision boundaries (and more) using Mondrian Trees\n\n\nGenerate data, fit and predict\n\n\nX_train, y_train, X_test, y_test = generate_toy_data()\nmtr = MondrianTreeRegressor(random_state=1, max_depth=2)\nmtr.fit(X_train, y_train)\ny_pred, y_std = mtr.predict(X_test, return_std=True)\n\n# This is a method that provides the weights given to each node while making predictions.\nweights = mtr.weighted_decision_path(X_test).toarray()\n\n\n\n\nFunction to plot bounds and decision boundaries at every node.\n\n\ndef plot_bounds_with_decision_boundaries(axis, X_tr, y_tr, X_te, y_te, tree_reg,\n                                         depth=0):\n    if depth > tree_reg.max_depth:\n        raise ValueError(\"Expected depth <= %d, got %d\" %\n                         (tree_reg.max_depth, depth))\n    colors = cycle(\"bgrcmyk\")\n    axis.set_ylim((-1.5, 1.5))\n    axis.plot(X_tr.ravel(), y_tr, \"ro\")\n    tree = tree_reg.tree_\n    x_l, x_u = axis.get_xlim()\n    if depth == 0:\n        axis.axvline(np.min(X_tr))\n        axis.axvline(np.max(X_tr))\n        axis.fill_between((x_l, x_u), -1.5, 1.5, alpha=0.2, color=next(colors))\n    else:\n        # All nodes upto a particular depth.\n        all_nodes = [0]\n        parent_nodes = [0]\n        curr_depth = 1\n        while curr_depth < depth:\n            curr_nodes = []\n            while parent_nodes:\n                nid = parent_nodes.pop()\n                curr_nodes.append(tree.children_left[nid])\n                curr_nodes.append(tree.children_right[nid])\n            parent_nodes = curr_nodes\n            all_nodes.extend(parent_nodes)\n            curr_depth += 1\n        thresholds = sorted([tree.threshold[node] for node in all_nodes])\n        thresh = [x_l] + thresholds + [x_u]\n        for start, end in zip(thresh[:-1], thresh[1:]):\n            axis.fill_between((start, end), -1.5, 1.5, alpha=0.2, color=next(colors))\n            X_1D = X_tr.ravel()\n            X_1D = X_1D[np.logical_and(X_1D > start, X_1D < end)]\n            axis.axvline(np.min(X_1D))\n            axis.axvline(np.max(X_1D))\n\n\n\n\nFunction to plot the weights at each node.\n\n\ndef plot_weights(axis, x_test, weights, tree_reg, depth=0):\n    tree = tree_reg.tree_\n    x_l, x_u = axis.get_xlim()\n    axis.set_ylim((0.0, 1.0))\n    axis.set_title(\"weights at depth %d\" % (depth))\n\n    if depth == 0:\n        axis.fill_between(x_test, 0.0, weights[:, 0], color=\"blue\")\n\n    else:\n        # Node ids at a particular depth.\n        parent_nodes = [0]\n        curr_depth = 0\n        while curr_depth < depth:\n            curr_nodes = []\n            while parent_nodes:\n                nid = parent_nodes.pop()\n                curr_nodes.append(tree.children_left[nid])\n                curr_nodes.append(tree.children_right[nid])\n            parent_nodes = curr_nodes\n            curr_depth += 1\n\n        weights = np.max(weights[:, parent_nodes], axis=1)\n        axis.fill_between(x_test, 0.0, weights, color=\"blue\")\n\n\n\n\nPlot all the things!!!\n\n\nfig, axes = plt.subplots(3, 2, sharex=True)\nfig.set_size_inches(18.5, 10.5)\nfor ax1, ax2 in axes:\n    ax1.set_xlim((-6.0, 6.0))\n    ax2.set_xlim((-6.0, 6.0))\nplot_weights(axes[0][0], X_test.ravel(), weights, mtr, depth=0)\nplot_weights(axes[1][0], X_test.ravel(), weights, mtr, depth=1)\nplot_weights(axes[2][0], X_test.ravel(), weights, mtr, depth=2)\nplot_bounds_with_decision_boundaries(\n    axes[0][1], X_train, y_train, X_test, y_test, mtr)\nplot_bounds_with_decision_boundaries(\n    axes[1][1], X_train, y_train, X_test, y_test, mtr, depth=1)\nplot_bounds_with_decision_boundaries(\n    axes[2][1], X_train, y_train, X_test, y_test, mtr, depth=2)\n\n\n\n\n\n\nInterpretation\n\n\nLet us interpret the plots from top to bottom.\n\n\n\n\n\n\nDepth zero: root\n\n\n\n\nWeights are zero within the bounding box.\n\n\nWeights start to increase as we move away from the bounding box, if we move far enough they will become one.\n\n\n\n\n\n\n\n\nDepth one\n\n\n\n\nBounding box of left child is smaller than that of the right. Hence the time of split of the left bounding box   is larger which makes the probability of separation higher and the weights are larger.\n\n\nThe small spike in the middle is because of the thin strip between both the bounding boxes. The probability of\n  separation here is non-zero and hence the weights are non-zero\n\n\n\n\n\n\n\n\nLeaf\n\n\n\n\nThe weights here are just so that the total weights sum up to one.\n\n\n\n\n\n\n\n\nConclusion\n\n\nIn conclusion, the mondrian tree regressor unlike standard decision tree implementations does not limit itself to\nthe leaf in making predictions. It takes into account the entire path from the root to the leaf and weighs it according to the distance from the bounding box in that node. This has some interesting properties such as falling back to the prior mean and variance for points far away from the training data.\n\n\nReferences:\n\n\n\n\nDecision Trees and Forests: A Probabilistic Perspective, Balaji Lakshminarayanan\n \nhttp://www.gatsby.ucl.ac.uk/~balaji/balaji-phd-thesis.pdf\n\n\nscikit-learn documentation\n\nhttp://scikit-learn.org/\n\n\nUnderstanding Random Forests, Gilles Louppe\n\nhttps://arxiv.org/abs/1407.7502\n\n\nMondrian Forests: Efficient Online Random Forests, Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n\nhttps://arxiv.org/abs/1406.2673\n\n\n\n\nAcknowledgements\n\n\nThis tutorial mainly arises from discussions with Gilles Louppe and Balaji Lakshminarayanan for which I am hugely grateful.",
            "title": "Mondrian Tree Tutorial"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#intuition-behind-mondrian-trees",
            "text": "This example provides intuition behind Mondrian Trees. In particular, the differences between existing decision tree algorithms and explanations of the tree construction and prediction will be highlighted.  import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import ExtraTreeRegressor\nfrom skgarden import MondrianTreeRegressor\nfrom itertools import cycle\n%matplotlib inline",
            "title": "Intuition behind Mondrian Trees"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#a-decision-tree-regressor",
            "text": "A decision tree is one of the easier-to-understand machine learning algorithms. While training, the input training space  X  is recursively partitioned into a number of rectangular subspaces. While predicting the label of a new point, one determines the rectangular subspace that it falls into and outputs the label representative of that subspace. This is usually the mean of the labels for a regression problem.  The rectangular subspaces are constructed in a greedy manner doing a binary partition at a time. Hence this can be determined by a split  S , a (f,  \\delta)  tuple where f is the feature index and  \\delta  is the threshold across which to split.  A point  x  for which  x[f]  is lesser than  \\delta  is placed in the left subspace and vice versa.",
            "title": "A decision tree regressor"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#so-how-is-each-split-done",
            "text": "The optimal split  S_{opt}  is determined by  (f_{opt}, \\delta_{opt} ) that creates children such that the weighted decrease in impurity is maximum. Mathematically, this is the combination that maximizes,  C=N_{parent} Imp_{Parent} - N_{left} Imp_{left} - N_{right} Imp_{right} , where  N_{parent}, N_{left}, N_{right}  are the number of samples in the parent, left and right subspaces.  In a standard decision tree,  S_{opt}  is found out by searching though all possible combinations of feature indices and values in the training data and simply returning the combination that minimizes C as described above. That sounds pretty expensive!  In an extremely randomized tree this is made much faster by the following procedure.   The feature indices of the candidate splits are determined by drawing  max_features  at random.  Then for each of these feature index, we select the split threshold by drawing uniformly between the bounds of that feature.   When the set of candidate splits are obtained, as before we just return that split that minimizes  C .",
            "title": "So how is each split done?"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#note",
            "text": "It is important to note that the actual reason is that while constructing an ensemble of trees, it makes sure that each tree constructed in an independent fashion. Decorrelating predictions in an ensemble is a key factor to achieve lower generalization error. For a highly unlikely corner case, if each tree in an ensemble is exactly the same, then there is no point constructing the ensemble.  Let us now generate some toy data to play with it in the remainder of this example. Here toy data, meaning a set of ten points that lie on a sine curve.  def generate_toy_data(n_points=10):\n    X_train = np.linspace(-np.pi, np.pi, n_points)\n    y_train = np.sin(X_train)\n    X_test = np.linspace(-6.0, 6.0, 100)\n    y_test = np.sin(X_test)\n    return np.expand_dims(X_train, axis=1), y_train, np.expand_dims(X_test, axis=1), y_test\n\nX_train, y_train, X_test, y_test = generate_toy_data()\nplt.plot(X_train.ravel(), y_train, \"ro\")\nplt.show()",
            "title": "Note:"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#plotting-decision-boundaries-using-erts",
            "text": "Let us now use scikit-learn's  ExtraTreeRegressor  to train on the generated toy data, predict on some unseen data and plot decision boundaries in the 1-D space. Also, we set the  max_depth  parameter to 2, which means there can be a maximum of 4 decision boundaries in the 1-D space.  def plot_1D_decision_boundaries(tree_reg, plt):\n    x_l, x_u = plt.xlim()\n    colors = cycle(\"bgrcmyk\")\n    split_thresh = tree_reg.tree_.threshold\n    split_thresh = sorted(split_thresh[split_thresh != -2])\n    split_thresh = [x_l] + split_thresh + [x_u]\n    for s in split_thresh:\n        plt.axvline(s, color=\"r\", linestyle=\"-\")\n\n    for x_l, x_u in zip(split_thresh[1:], split_thresh[:-1]):\n        plt.fill_between((x_l, x_u), -1.5, 1.5, alpha=0.2, color=next(colors))\n\netr = ExtraTreeRegressor(random_state=2, max_depth=2)\netr.fit(X_train, y_train)\ny_pred = etr.predict(X_test)\nplt.plot(X_train.ravel(), y_train, \"ro\")\nplt.plot(X_test.ravel(), y_pred)\nplt.ylim((-1.5, 1.5))\nplot_1D_decision_boundaries(etr, plt)\nplt.show()   The blue line represents the mean prediction in every region of the decision space. So if a point lies between -6 and -3, we predict y to be 0.0 and so on.  There are two things to notice.   As we move away from the training data, the predicted mean remains constant which is determined by the decision split at the bounds of the space. But in reality, we are unsure about the target value to predict and would like to fall back on some prior mean.  In an extremely randomized tree, this issue is not confined to subspaces at the edge of the training subspace. Even if the green subspace was not at the edge, we are unsure about the region from 4 to 6 since the train points are confined till 4.0   The mondrian tree solves this problem in a very intelligent way.",
            "title": "Plotting decision boundaries using ERT's"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#mondrian-tree",
            "text": "",
            "title": "Mondrian Tree"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#train-mode",
            "text": "The split tuple (f,  \\delta)  is decided independently of the target or the decrease in impurity! Yes, that is right. When all the features are of same scale and are equally important, this is same as an extremely randomized tree with  max_features  set to 1.   The split feature index  f  is drawn with a probability proportional to  u_b[f] - l_b[f]  where  u_b  and  l_b  and the upper and lower bounds of all the features.  After fixing the feature index, the split threshold  \\delta  is then drawn from a uniform distribution with limits  l_b ,  u_b .   The intuition being that a feature that has a huge difference between the bounds is likelier to be an \"important\" feature. Every subspace  j  in a mondrian tree also stores information about.   The upper and lower bounds of all the features in that particular node or the bounding box as determined by the training data. For example, in the green subspace above it stores ((1.96, 3.0),)  The time of split  \\tau  which is drawn from an exponential with mean  \\sum_{f=1}^D(u_b[f] - l_b[f]) . Smaller the value of tau, larger is the bounding box.",
            "title": "Train mode"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#note_1",
            "text": "The time of split can be viewed as weighted depth. Imagine that edges between parent and child nodes are associated with a non-negative weight. The time of split at a node is the sum of the weights along the path from the root to the node. If weights are all 1, the time of split is the depth of the node.",
            "title": "Note:"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#prediction-mode",
            "text": "From now on, we will use the terms node and subspace interchangeably. The subspace that a new point ends up in is the leaf, the entire training space is the root and every binary partition results in two nodes.  Recall that for a decision tree, computing the prediction for a new point is fairly straightforward. Find the leaf node that a new point lands in and output the mean.  The prediction step of a Mondrian Tree is a bit more complicated. It takes into account all the nodes in the path of a new point from the root to the leaf for making a prediction. This formulation allows us the flexibility to weigh the nodes on the basis of how sure/unsure we are about the prediction in that particular node.  Mathematically, the distribution of  P(Y | X)  is given by   P(Y | X) = \\sum_{j} w_j \\mathcal{N} (m_j, v_j)   where the summation is across all the nodes in the path from the root to the leaf. The mean prediction becomes  \\sum_{j} w_j m_j",
            "title": "Prediction mode"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#computing-the-weights",
            "text": "Assume  p_j(x)  denote the probability of a new point splitting away from a node. That is higher the probability, the farther away it is from the bounding box at that node.  A point which is within the bounds at any node, the probability of separation should be zero. This means we can obtain a better estimate about the prediction from its child node.  Also, the node at which the new point starts to split away is the node that we are most confident about. So this should be given a high weight.  Formally, the weights are computed like this.  If  j  is not a leaf:   w_j(x) = p_j(x) \\prod_{k \\in anc(j)} (1 - p_k(x))   If  j  is a leaf, to make the weights sum up to one.   w_j(x) = 1 - \\sum_{k \\in anc(j)} w_k(x)    w_j  can be decomposed into two factors,  p_j (x) and  \\prod_{k \\in anc(j)} (1 - p_k(x)) . The first one being the probability of splitting away at that particular node and the second one being the probability of not splitting away till it reaches that node. We can observe that for  x  that is completely within the bounds of a node,  w_j(x)  becomes zero and for a point where it starts branching off,  w_j(x) = p_j(x)",
            "title": "Computing the weights."
        },
        {
            "location": "/examples/MondrianTreeRegressor/#computing-the-probability-of-separation",
            "text": "We come to the final piece in the jigsaw puzzle, that is computing the probability of separation  p_j(x)  of each node. This is computed in the following way.    \\Delta_{j} = \\tau_{j} - \\tau_{parent(j)}    \\eta_{j}(x) = \\sum_{f}(\\max(x[f] - u_{bj}[f], 0) + \\max(0, l_{bj}[f] - x[f]))    p_j(x) = 1 - e^{-\\Delta_{j} \\eta_{j}(x))}    Let us take some time to stare at these equations and understand what they mean.    p_j(x)  is high when  \\eta_{j}(x)  is high. As  \\eta_{j}(x)  approaches infinity,  p_j(x)  approaches zero. This means that when the point is far away from the bounding box of the node, the probability of separation becomes high.   p_j(x)  is high when  \\Delta{j}  is high. This means when the bounding box of a node is small as compared to the bounding box of its parent, the probability of separation becomes high.  For a point in the training data,  p_j(x)  becomes zero for all nodes other than the leaf since the point is within the bounding box at all nodes. The leaf then has a weightage of 1.0 and this reduces to a standard decision tree prediction.  For a point far away from the training data,  p_{root}(x)  (and  w_{root}(x) ) approach one and hence the weights of the other nodes in the path from the root to the leaf approach zero. This means  P(Y | X) = \\mathcal{N}(m, v)  where  m  and  v  are the empirical mean and variance of the training data.",
            "title": "Computing the probability of separation."
        },
        {
            "location": "/examples/MondrianTreeRegressor/#plotting-decision-boundaries-and-more-using-mondrian-trees",
            "text": "",
            "title": "Plotting decision boundaries (and more) using Mondrian Trees"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#generate-data-fit-and-predict",
            "text": "X_train, y_train, X_test, y_test = generate_toy_data()\nmtr = MondrianTreeRegressor(random_state=1, max_depth=2)\nmtr.fit(X_train, y_train)\ny_pred, y_std = mtr.predict(X_test, return_std=True)\n\n# This is a method that provides the weights given to each node while making predictions.\nweights = mtr.weighted_decision_path(X_test).toarray()",
            "title": "Generate data, fit and predict"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#function-to-plot-bounds-and-decision-boundaries-at-every-node",
            "text": "def plot_bounds_with_decision_boundaries(axis, X_tr, y_tr, X_te, y_te, tree_reg,\n                                         depth=0):\n    if depth > tree_reg.max_depth:\n        raise ValueError(\"Expected depth <= %d, got %d\" %\n                         (tree_reg.max_depth, depth))\n    colors = cycle(\"bgrcmyk\")\n    axis.set_ylim((-1.5, 1.5))\n    axis.plot(X_tr.ravel(), y_tr, \"ro\")\n    tree = tree_reg.tree_\n    x_l, x_u = axis.get_xlim()\n    if depth == 0:\n        axis.axvline(np.min(X_tr))\n        axis.axvline(np.max(X_tr))\n        axis.fill_between((x_l, x_u), -1.5, 1.5, alpha=0.2, color=next(colors))\n    else:\n        # All nodes upto a particular depth.\n        all_nodes = [0]\n        parent_nodes = [0]\n        curr_depth = 1\n        while curr_depth < depth:\n            curr_nodes = []\n            while parent_nodes:\n                nid = parent_nodes.pop()\n                curr_nodes.append(tree.children_left[nid])\n                curr_nodes.append(tree.children_right[nid])\n            parent_nodes = curr_nodes\n            all_nodes.extend(parent_nodes)\n            curr_depth += 1\n        thresholds = sorted([tree.threshold[node] for node in all_nodes])\n        thresh = [x_l] + thresholds + [x_u]\n        for start, end in zip(thresh[:-1], thresh[1:]):\n            axis.fill_between((start, end), -1.5, 1.5, alpha=0.2, color=next(colors))\n            X_1D = X_tr.ravel()\n            X_1D = X_1D[np.logical_and(X_1D > start, X_1D < end)]\n            axis.axvline(np.min(X_1D))\n            axis.axvline(np.max(X_1D))",
            "title": "Function to plot bounds and decision boundaries at every node."
        },
        {
            "location": "/examples/MondrianTreeRegressor/#function-to-plot-the-weights-at-each-node",
            "text": "def plot_weights(axis, x_test, weights, tree_reg, depth=0):\n    tree = tree_reg.tree_\n    x_l, x_u = axis.get_xlim()\n    axis.set_ylim((0.0, 1.0))\n    axis.set_title(\"weights at depth %d\" % (depth))\n\n    if depth == 0:\n        axis.fill_between(x_test, 0.0, weights[:, 0], color=\"blue\")\n\n    else:\n        # Node ids at a particular depth.\n        parent_nodes = [0]\n        curr_depth = 0\n        while curr_depth < depth:\n            curr_nodes = []\n            while parent_nodes:\n                nid = parent_nodes.pop()\n                curr_nodes.append(tree.children_left[nid])\n                curr_nodes.append(tree.children_right[nid])\n            parent_nodes = curr_nodes\n            curr_depth += 1\n\n        weights = np.max(weights[:, parent_nodes], axis=1)\n        axis.fill_between(x_test, 0.0, weights, color=\"blue\")",
            "title": "Function to plot the weights at each node."
        },
        {
            "location": "/examples/MondrianTreeRegressor/#plot-all-the-things",
            "text": "fig, axes = plt.subplots(3, 2, sharex=True)\nfig.set_size_inches(18.5, 10.5)\nfor ax1, ax2 in axes:\n    ax1.set_xlim((-6.0, 6.0))\n    ax2.set_xlim((-6.0, 6.0))\nplot_weights(axes[0][0], X_test.ravel(), weights, mtr, depth=0)\nplot_weights(axes[1][0], X_test.ravel(), weights, mtr, depth=1)\nplot_weights(axes[2][0], X_test.ravel(), weights, mtr, depth=2)\nplot_bounds_with_decision_boundaries(\n    axes[0][1], X_train, y_train, X_test, y_test, mtr)\nplot_bounds_with_decision_boundaries(\n    axes[1][1], X_train, y_train, X_test, y_test, mtr, depth=1)\nplot_bounds_with_decision_boundaries(\n    axes[2][1], X_train, y_train, X_test, y_test, mtr, depth=2)",
            "title": "Plot all the things!!!"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#interpretation",
            "text": "Let us interpret the plots from top to bottom.    Depth zero: root   Weights are zero within the bounding box.  Weights start to increase as we move away from the bounding box, if we move far enough they will become one.     Depth one   Bounding box of left child is smaller than that of the right. Hence the time of split of the left bounding box   is larger which makes the probability of separation higher and the weights are larger.  The small spike in the middle is because of the thin strip between both the bounding boxes. The probability of\n  separation here is non-zero and hence the weights are non-zero     Leaf   The weights here are just so that the total weights sum up to one.",
            "title": "Interpretation"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#conclusion",
            "text": "In conclusion, the mondrian tree regressor unlike standard decision tree implementations does not limit itself to\nthe leaf in making predictions. It takes into account the entire path from the root to the leaf and weighs it according to the distance from the bounding box in that node. This has some interesting properties such as falling back to the prior mean and variance for points far away from the training data.",
            "title": "Conclusion"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#references",
            "text": "Decision Trees and Forests: A Probabilistic Perspective, Balaji Lakshminarayanan\n  http://www.gatsby.ucl.ac.uk/~balaji/balaji-phd-thesis.pdf  scikit-learn documentation http://scikit-learn.org/  Understanding Random Forests, Gilles Louppe https://arxiv.org/abs/1407.7502  Mondrian Forests: Efficient Online Random Forests, Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh https://arxiv.org/abs/1406.2673",
            "title": "References:"
        },
        {
            "location": "/examples/MondrianTreeRegressor/#acknowledgements",
            "text": "This tutorial mainly arises from discussions with Gilles Louppe and Balaji Lakshminarayanan for which I am hugely grateful.",
            "title": "Acknowledgements"
        },
        {
            "location": "/examples/QuantileRegressionForests/",
            "text": "Quantile Regression Forests\n\n\nIntroduction\n\n\nMost estimators during prediction return \nE(Y | X)\n, which can be interpreted as the\nanswer to the question, what is the expected value of your output given the input?\n\n\nQuantile methods, return \ny\n at \nq\n for which \nF(Y=y|X) = q\n where \nq\n\nis the percentile and \ny\n is the quantile. One quick\nuse-case where this is useful is when there are a number of outliers which can influence\nthe conditional mean. It is sometimes important to obtain estimates at different\npercentiles, (when grading on a curve is done for instance.)\n\n\nNote: Some machine learning models also return the entire distribution of \nP(Y | X)\n.\nsuch as \nGaussian Processes\n and\n\nMondrian Forests\n. A useful application is in\n\nhyperparameter optimisation\n, where the conditional distribution \nP(Y|X)\n is\nnecessary to balance the exploitation and exploration.\n\n\nQuantile Decision Trees\n\n\nIt is fairly straightforward to extend a standard decision tree to provide\npredictions at percentiles. When a decision tree is fit, the trick is to\nstore not only the sufficient statistics of the target at the leaf node\nsuch as the mean and variance but also all the target values in the leaf node.\nAt prediction, these are used to compute empirical quantile estimates.\n\n\nLet's say, the parameter \nmin_samples_leaf\n is set to 5, then for a new sample\n\nX\n, the 5 samples in the leaf are given equal weight while determining\n\nY | X\n at different quantiles. If \nmin_samples_leaf\n is set to 1, then\nthe expectation equals the quantile at every percentile.\n\n\nNote: The empirical estimation of quantiles can be done in many ways.\n\nscikit-garden\n, relies on this \nWeighted Percentile Method\n\n\nQuantile Regression Forests.\n\n\nThe same approach can be extended to RandomForests. To estimate \nF(Y=y|x) = q\n\neach target value in \ny_train\n is given a weight. Formally, the weight given to\n\ny_train[j]\n while estimating the quantile is \n \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}(y_j \\in L(x))}{\\sum_{i=1}^N \\mathbb{1}(y_i \\in L(x))}\n where \nL(x)\n denotes the leaf\nthat \nx\n falls into.\n\n\nInformally, what it means that for a new unknown sample, we first find the leaf that it\nfalls into at each tree. Then for each \n(X, y)\n in the training data, a weight is\ngiven to \ny\n at each tree in the following manner.\n\n\n\n\nIf it is in the same leaf as the new sample, then the weight is the fraction\nof samples in the same leaf.\n\n\nIf not, then the weight is zero.\n\n\n\n\nThese weights for each \ny\n are summed up across all trees and averaged.\nNow since we have an array of target values and an array of weights corresponding\nto these target values, we can use this to measure empirical quantile estimates.\n\n\nExample\n\n\nWe will now use the \nExtraTreesQuantileRegressor\n from scikit-garden to plot\nprediction intervals on the boston dataset.\n\n\nImport necessary things\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom skgarden import RandomForestQuantileRegressor\n\n\n\n\nLoad data and the necessary estimators. Note that \nmin_samples_split\n\nis set to 10 and the cross-validation is 5-split.\n\n\nboston = load_boston()\nX, y = boston.data, boston.target\nkf = KFold(n_splits=5, random_state=0)\nrfqr = RandomForestQuantileRegressor(\n    random_state=0, min_samples_split=10, n_estimators=1000)\n\n\n\n\nStore the quantiles at the 98.5th and 2.5th percentile.\n\n\ny_true_all = []\nlower = []\nupper = []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (\n        X[train_index], X[test_index], y[train_index], y[test_index])\n\n    rfqr.set_params(max_features=X_train.shape[1] // 3)\n    rfqr.fit(X_train, y_train)\n    y_true_all = np.concatenate((y_true_all, y_test))\n    upper = np.concatenate((upper, rfqr.predict(X_test, quantile=98.5)))\n    lower = np.concatenate((lower, rfqr.predict(X_test, quantile=2.5)))\n\ninterval = upper - lower\nsort_ind = np.argsort(interval)\ny_true_all = y_true_all[sort_ind]\nupper = upper[sort_ind]\nlower = lower[sort_ind]\nmean = (upper + lower) / 2\n\n# Center such that the mean of the prediction interval is at 0.0\ny_true_all -= mean\nupper -= mean\nlower -= mean\n\n\n\n\nPlot the prediction intervals, the original target values. We see that most\nof the samples lie within the 95 p.c prediction interval.\n\n\nplt.plot(y_true_all, \"ro\")\nplt.fill_between(\n    np.arange(len(upper)), lower, upper, alpha=0.2, color=\"r\",\n    label=\"Pred. interval\")\nplt.xlabel(\"Ordered samples.\")\nplt.ylabel(\"Values and prediction intervals.\")\nplt.xlim([0, 500])\nplt.show()",
            "title": "Quantile Regression Forests"
        },
        {
            "location": "/examples/QuantileRegressionForests/#quantile-regression-forests",
            "text": "",
            "title": "Quantile Regression Forests"
        },
        {
            "location": "/examples/QuantileRegressionForests/#introduction",
            "text": "Most estimators during prediction return  E(Y | X) , which can be interpreted as the\nanswer to the question, what is the expected value of your output given the input?  Quantile methods, return  y  at  q  for which  F(Y=y|X) = q  where  q \nis the percentile and  y  is the quantile. One quick\nuse-case where this is useful is when there are a number of outliers which can influence\nthe conditional mean. It is sometimes important to obtain estimates at different\npercentiles, (when grading on a curve is done for instance.)  Note: Some machine learning models also return the entire distribution of  P(Y | X) .\nsuch as  Gaussian Processes  and Mondrian Forests . A useful application is in hyperparameter optimisation , where the conditional distribution  P(Y|X)  is\nnecessary to balance the exploitation and exploration.",
            "title": "Introduction"
        },
        {
            "location": "/examples/QuantileRegressionForests/#quantile-decision-trees",
            "text": "It is fairly straightforward to extend a standard decision tree to provide\npredictions at percentiles. When a decision tree is fit, the trick is to\nstore not only the sufficient statistics of the target at the leaf node\nsuch as the mean and variance but also all the target values in the leaf node.\nAt prediction, these are used to compute empirical quantile estimates.  Let's say, the parameter  min_samples_leaf  is set to 5, then for a new sample X , the 5 samples in the leaf are given equal weight while determining Y | X  at different quantiles. If  min_samples_leaf  is set to 1, then\nthe expectation equals the quantile at every percentile.  Note: The empirical estimation of quantiles can be done in many ways. scikit-garden , relies on this  Weighted Percentile Method",
            "title": "Quantile Decision Trees"
        },
        {
            "location": "/examples/QuantileRegressionForests/#quantile-regression-forests_1",
            "text": "The same approach can be extended to RandomForests. To estimate  F(Y=y|x) = q \neach target value in  y_train  is given a weight. Formally, the weight given to y_train[j]  while estimating the quantile is   \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}(y_j \\in L(x))}{\\sum_{i=1}^N \\mathbb{1}(y_i \\in L(x))}  where  L(x)  denotes the leaf\nthat  x  falls into.  Informally, what it means that for a new unknown sample, we first find the leaf that it\nfalls into at each tree. Then for each  (X, y)  in the training data, a weight is\ngiven to  y  at each tree in the following manner.   If it is in the same leaf as the new sample, then the weight is the fraction\nof samples in the same leaf.  If not, then the weight is zero.   These weights for each  y  are summed up across all trees and averaged.\nNow since we have an array of target values and an array of weights corresponding\nto these target values, we can use this to measure empirical quantile estimates.",
            "title": "Quantile Regression Forests."
        },
        {
            "location": "/examples/QuantileRegressionForests/#example",
            "text": "We will now use the  ExtraTreesQuantileRegressor  from scikit-garden to plot\nprediction intervals on the boston dataset.  Import necessary things  import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom skgarden import RandomForestQuantileRegressor  Load data and the necessary estimators. Note that  min_samples_split \nis set to 10 and the cross-validation is 5-split.  boston = load_boston()\nX, y = boston.data, boston.target\nkf = KFold(n_splits=5, random_state=0)\nrfqr = RandomForestQuantileRegressor(\n    random_state=0, min_samples_split=10, n_estimators=1000)  Store the quantiles at the 98.5th and 2.5th percentile.  y_true_all = []\nlower = []\nupper = []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (\n        X[train_index], X[test_index], y[train_index], y[test_index])\n\n    rfqr.set_params(max_features=X_train.shape[1] // 3)\n    rfqr.fit(X_train, y_train)\n    y_true_all = np.concatenate((y_true_all, y_test))\n    upper = np.concatenate((upper, rfqr.predict(X_test, quantile=98.5)))\n    lower = np.concatenate((lower, rfqr.predict(X_test, quantile=2.5)))\n\ninterval = upper - lower\nsort_ind = np.argsort(interval)\ny_true_all = y_true_all[sort_ind]\nupper = upper[sort_ind]\nlower = lower[sort_ind]\nmean = (upper + lower) / 2\n\n# Center such that the mean of the prediction interval is at 0.0\ny_true_all -= mean\nupper -= mean\nlower -= mean  Plot the prediction intervals, the original target values. We see that most\nof the samples lie within the 95 p.c prediction interval.  plt.plot(y_true_all, \"ro\")\nplt.fill_between(\n    np.arange(len(upper)), lower, upper, alpha=0.2, color=\"r\",\n    label=\"Pred. interval\")\nplt.xlabel(\"Ordered samples.\")\nplt.ylabel(\"Values and prediction intervals.\")\nplt.xlim([0, 500])\nplt.show()",
            "title": "Example"
        }
    ]
}